{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "OQ4zTAe6s78f"
            },
            "source": [
                "# 9章\n",
                "\n",
                "- 本章で用いる「[日本語Wikipedia入力誤りデータセット](https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9EWikipedia%E5%85%A5%E5%8A%9B%E8%AA%A4%E3%82%8A%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88)」は現在バージョン2が公開されていますが、本章では[バージョン1](https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9EWikipedia%E5%85%A5%E5%8A%9B%E8%AA%A4%E3%82%8A%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88v1)を用いています。"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "本章では、文章校正タスクのうち漢字の誤変換タスクを扱う。BERTでは、文章校正をトークンの分類問題として扱うことで実装できる。<br>\n",
                "正しいトークンについては同じトークンを、間違っていると考えられるトークンにはBERTの語彙の中から正しいと予測したトークンを返す。\n",
                "\n",
                "|誤変換|正しい文章|\n",
                "|:-|:-|\n",
                "|優勝|優勝|\n",
                "|トロフィー|トロフィー|\n",
                "|を|を|\n",
                "|変換|返還|\n",
                "|し|し|\n",
                "|た|た|\n",
                "|。|。|\n",
                "\n",
                "一方で、以下のようにこの方法では扱えない文章が存在する。\n",
                "\n",
                "|誤変換|正しい文章|\n",
                "|:-|:-|\n",
                "|投|当初|\n",
                "|##書|は|\n",
                "|は|、|\n",
                "|、|実行|\n",
                "|実行|を|\n",
                "\n",
                "本章では、上記のような文章を取り扱わないこととする。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-3\n",
                "\n",
                "import os\n",
                "import random\n",
                "import unicodedata\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import pprint\n",
                "import math\n",
                "\n",
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
                "import pytorch_lightning as pl"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "トークナイザを定義する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-4\n",
                "\n",
                "class SC_tokenizer(BertJapaneseTokenizer):\n",
                "    def encode_plus_tagged(\n",
                "        self,\n",
                "        wrong_text: str,\n",
                "        correct_text: str,\n",
                "        max_length: int = 128,\n",
                "    ) -> dict:\n",
                "        \"\"\"\n",
                "        ファインチューニング用\n",
                "        誤変換を含む文章と正しい文章を渡し符号化し、\n",
                "        誤変換文章のlabelsを正しい文章のinput_idsに置き換える\n",
                "        \"\"\"\n",
                "        # 文章から直接符号化\n",
                "        # tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)で\n",
                "        # インスタンス化したtokinzer(text)関数と同等\n",
                "        wrong_encoding = self(\n",
                "            wrong_text, max_length=max_length, padding='max_length', truncation=True\n",
                "        )\n",
                "        correct_encoding = self(\n",
                "            correct_text, max_length=max_length, padding='max_length', truncation=True\n",
                "        )\n",
                "        # 正しい文章の符号をラベルとする\n",
                "        wrong_encoding['labels'] = correct_encoding['input_ids']\n",
                "\n",
                "        return wrong_encoding\n",
                "    \n",
                "    def encode_plus_untagged(\n",
                "        self,\n",
                "        text: str,\n",
                "        max_length: int = None,\n",
                "        return_tensors: str = 'pt',\n",
                "    ) -> (dict, list):\n",
                "        \"\"\"\n",
                "        文章をトークン化し、それぞれのトークンと文章中の文字列を対応付ける\n",
                "        推論時にトークンごとのラベルを予測し、最終的に固有表現に変換する\n",
                "        未知語や文章中の空白(MeCabにより消去される)に対しての処理が必要となる\n",
                "        そのため、各トークンが元の文章のどの位置にあったかを特定しておく\n",
                "        \"\"\"\n",
                "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
                "        tokens = []\n",
                "        tokens_original = []\n",
                "        # 単語をサブワードに分割してlistに格納\n",
                "        for word in words:\n",
                "            tokens_subword = self.subword_tokenizer.tokenize(word)\n",
                "            tokens.extend(tokens_subword)\n",
                "            # 未知語対応\n",
                "            if tokens_subword[0] == '[UNK]':\n",
                "                tokens_original.append(word)\n",
                "            else:\n",
                "                tokens_original.extend([token.replace(\"##\", \"\") for token in tokens_subword])\n",
                "        \n",
                "        # トークンが文章中のどの位置にあるかを走査する\n",
                "        position = 0\n",
                "        spans = []\n",
                "        for token in tokens_original:\n",
                "            length = len(token)\n",
                "            while True:\n",
                "                if token != text[position: position + length]:\n",
                "                    position += 1\n",
                "                else:\n",
                "                    spans.append([position, position + length])\n",
                "                    position += length\n",
                "                    break\n",
                "        \n",
                "        # トークンをID化する\n",
                "        input_ids = self.convert_tokens_to_ids(tokens)\n",
                "        # トークンIDを符号化する\n",
                "        encoding = self.prepare_for_model(\n",
                "            input_ids, \n",
                "            max_length=max_length,\n",
                "            padding = 'max_length' if max_length else False,\n",
                "            truncation = True if max_length else False,\n",
                "        )\n",
                "        sequence_length = len(encoding['input_ids']) # 符号化した文章の長さ\n",
                "        # 先頭トークン[CLS]用のspanを追加する\n",
                "        # このとき、次の[SEP]トークンを一緒に削除しておく\n",
                "        spans = [[-1, -1]] + spans[:sequence_length - 2]\n",
                "        # 末尾トークン[SEP]、末尾の空トークン[PAD]用のspanを追加する\n",
                "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
                "\n",
                "        # 引数に応じてtorch.Tensor型に返還\n",
                "        # 次元を追加する必要がある\n",
                "        if return_tensors == 'pt':\n",
                "            encoding = {key: torch.tensor([value]) for key, value in encoding.items()}\n",
                "        \n",
                "        return (encoding, spans)\n",
                "    \n",
                "    def convert_output_to_text(\n",
                "        self, \n",
                "        text: str,\n",
                "        labels_arg: list,\n",
                "        spans_arg: list,\n",
                "    ) -> str:\n",
                "        \"\"\"\n",
                "        文章、各トークンのラベルの予測値、文章中での位置から\n",
                "        予測された文章に変換する\n",
                "        \"\"\"\n",
                "        # 文章の長さチェック\n",
                "        assert len(labels_arg) == len(spans_arg)\n",
                "\n",
                "        # 特殊トークンを取り除く\n",
                "        labels = []\n",
                "        spans = []\n",
                "        for label, span in zip(labels_arg, spans_arg):\n",
                "            if span[0] != -1:\n",
                "                labels.append(label)\n",
                "                spans.append(span)\n",
                "        \n",
                "        # モデルが予測した文章を生成する\n",
                "        text_pred = \"\"\n",
                "        position = 0\n",
                "        for label, span in zip(labels, spans):\n",
                "            start, end = span\n",
                "            # 空白文字の処理\n",
                "            if position != start:\n",
                "                text_pred += text[position: start]\n",
                "            token_pred = self.convert_ids_to_tokens(label) # labelをトークン化\n",
                "            token_pred = token_pred.replace(\"##\", \"\") # サブワードの##を削除\n",
                "            token_pred = unicodedata.normalize('NFKC', token_pred) # 文字列の正規化\n",
                "            text_pred += token_pred\n",
                "            position = end\n",
                "        return text_pred"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "定義したトークナイザの動きを確認する。\n",
                "\n",
                "モデルとトークナイザの呼び出し"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "HBTlINuoGXNc"
            },
            "outputs": [],
            "source": [
                "# 9-5\n",
                "\n",
                "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
                "tokenizer = SC_tokenizer.from_pretrained(model_name)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`encode_plus_tagged()`メソッドは、誤変換した文章と正しい文章が渡されたら、誤変換した文章を符号化し`labels`に正しい文章のトークンIDを付与する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-6\n",
                "\n",
                "wrong_text = \"優勝トロフィーを変換した\"\n",
                "correct_text = \"優勝トロフィーを返還した\"\n",
                "\n",
                "encoding = tokenizer.encode_plus_tagged(\n",
                "    wrong_text, correct_text, max_length=12,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pprint.pprint(encoding)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`encode_plus_untagged()`メソッドは、文章を符号化し、空白や未知語を考慮した上でそれぞれのトークンの位置を返す。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-7\n",
                "\n",
                "wrong_text = \"優勝トロフィーを変換した\"\n",
                "encoding, spans = tokenizer.encode_plus_untagged(\n",
                "    wrong_text, return_tensors='pt'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spans"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`convert_output_to_text()`関数は、文章とラベル列、各トークンの文章中の位置から予測した文章を出力する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-8\n",
                "\n",
                "labels_pred = [2, 759, 18204, 11, 8274, 15, 10, 3]\n",
                "text_pred = tokenizer.convert_output_to_text(\n",
                "    wrong_text, labels_pred, spans\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_pred"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9.2 BERTにおける実装"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`transformers.BertForMaskedLM`クラスを用いる。このクラスは各トークンに対してその市に入るトークンを語彙の中から選ぶもので、これは`transformers.BertForTokenClassification`でラベル数を語彙数としたものと入出力関係が同じと考えることができる。<br>\n",
                "一方で、`transformers.BertForMaskedLM`は、事前学習でランダムに選ばれたトークンを`[MASK]`もしくはランダムなトークンに置き換え、またはそのままで、元のトークンが何であったかを予測するという学習が行われる。これは、文章校正タスクと類似しており、分類器の初期パラメータとして事前学習により得られたものを用いる`transformers.BertForMaskedLM`のほうが、`transformers.BertForTokenClassification`と比較してある程度妥当なパラメータを備えていることが期待されるためである。これにより、ファインチューニングの学習時間の短縮が期待できる。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-9\n",
                "\n",
                "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
                "model = BertForMaskedLM.from_pretrained(model_name)\n",
                "model = model.cuda()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "文章の符号化から予測された文章の出力までは、以下のように実装する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-10\n",
                "\n",
                "text = \"優勝トロフィーを変換した。\"\n",
                "\n",
                "# 符号化およびトークンの文章中の位置を取得\n",
                "encoding, spans = tokenizer.encode_plus_untagged(\n",
                "    text, return_tensors='pt'\n",
                ")\n",
                "encoding = {key: value.cuda() for key, value in encoding.items()}\n",
                "\n",
                "# BERTに入力\n",
                "with torch.no_grad():\n",
                "    output = model(**encoding)\n",
                "scores = output.logits\n",
                "labels_pred = scores[0].argmax(axis=1).cpu().numpy().tolist()\n",
                "\n",
                "text_pred = tokenizer.convert_output_to_text(\n",
                "    text, labels_pred, spans\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_pred"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "次に、誤変換した文章と正しい文章をBERTに入力して損失を計算する。入力時に、それぞれの文章を符号化し、正しい文章についてはラベル列を正とする。`tokenizer.encode_plus_tagged()`を用いることで、ラベル列を含んだデータを作成できる。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-11\n",
                "\n",
                "data = [\n",
                "    {\n",
                "        'wrong_text': '優勝トロフィーを変換した。',\n",
                "        'correct_text': '優勝トロフィーを返還した。',\n",
                "    },\n",
                "    {\n",
                "        'wrong_text': '人と森は強制している。',\n",
                "        'correct_text': '人と森は共生している。',\n",
                "    }\n",
                "]\n",
                "\n",
                "# データの符号化\n",
                "max_length = 32\n",
                "dataset_for_loader = []\n",
                "for sample in data:\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    encoding = tokenizer.encode_plus_tagged(\n",
                "        wrong_text, correct_text, max_length=max_length\n",
                "    )\n",
                "    encoding = {key: torch.tensor(value) for key, value in encoding.items()}\n",
                "    dataset_for_loader.append(encoding)\n",
                "\n",
                "dataloader = DataLoader(dataset_for_loader, batch_size=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for batch in dataloader:\n",
                "    encoding = {key: value.cuda() for key, value in batch.items()}\n",
                "    output = model(**encoding)\n",
                "    loss = output.loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "loss"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9.4 日本語Wikipedia誤りデータセット\n",
                "\n",
                "本章では、京都大学の言語メディア研究室が作成した[日本語Wikipedia誤りデータセット](\"https://nlp.ist.i.kyoto-u.ac.jp/?日本語Wikipedia入力誤りデータセット\")を用いて学習を行う。このデータセットは、日本語Wikipediaの差分から間違った文章と正しい文章のペアを抽出している。\n",
                "\n",
                "まずは、データセット(v2.0)をダウンロードする。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-2\n",
                "\n",
                "os.makedirs(\"data/chapter9\", exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-12\n",
                "\n",
                "!curl -o \"data/chapter9/jwtd.tar.gz\" \"https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd_v2.0.tar.gz&name=JWTDv2.0.tar.gz\"\n",
                "!cd \"data/chapter9\" && tar -zxf \"jwtd.tar.gz\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "本データセットはJSON形式で、`pre_text`に修正前の文章、`post_text`に修正後の文章が格納されている。\n",
                "さらに、`diffs`配列の中の`category`に入力誤りの種類(誤字、脱字など)が、`pre_str`が修正前、`post_str`が修正後の単語が格納されている。\n",
                "\n",
                "本章では漢字誤変換のみを扱う。`category`が`kanji-conversion_a`となっているもののみを利用する。\n",
                "\n",
                "まずデータを読み込む。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-13\n",
                "\n",
                "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
                "tokenizer = SC_tokenizer.from_pretrained(model_name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "category_type = 'kanji-conversion_a'\n",
                "\n",
                "def create_dataset(data_df: pd.DataFrame) -> dict:\n",
                "    def _check_category(lst: list) -> bool:\n",
                "        \"\"\"\n",
                "        data_df.diffsを走査して、\n",
                "        - listの長さが2以下\n",
                "        - すべてのcategoryがkanji-conversion_a\n",
                "        のすべてを満たすときにTrue、そうでないときにFalseを返す\n",
                "        \"\"\"\n",
                "        checked_lst = [dct['category'] == category_type for dct in lst]\n",
                "        category_bool =  math.prod(checked_lst) # リスト内を掛け算してFalseがあれば0が返る\n",
                "        length_bool = len(checked_lst) <= 2\n",
                "        return bool(category_bool * length_bool)\n",
                "\n",
                "    def _normalize(text: str) -> str:\n",
                "        \"\"\"\n",
                "        文字列の正規化を行う\n",
                "        \"\"\"\n",
                "        text = text.strip() # 改行文字や全角スペースを取り除く\n",
                "        return unicodedata.normalize('NFKC', text) # NFKCで正規化\n",
                "\n",
                "    def check_token_count(row):\n",
                "        pre_text_tokens = tokenizer.tokenize(row['pre_text'])\n",
                "        post_text_tokens = tokenizer.tokenize(row['post_text'])\n",
                "        if len(pre_text_tokens) != len(post_text_tokens):\n",
                "            return False\n",
                "        \n",
                "        diff_count = 0\n",
                "        threshold_count = 2\n",
                "        for pre_text_token, post_text_token in zip(pre_text_tokens, post_text_tokens):\n",
                "            if pre_text_token != post_text_token:\n",
                "                diff_count += 1\n",
                "                if diff_count > threshold_count:\n",
                "                    return False\n",
                "        else:\n",
                "            return True\n",
                "\n",
                "    # data_df.diffsのtypeをobject -> listに変換\n",
                "    data_df.diffs = data_df.diffs.apply(lambda x: list(x))\n",
                "\n",
                "    # data_dfからcategoryがkanji-conversion_aかつ誤変換が2以下の文章のみを抜き出す\n",
                "    data_df = data_df[data_df['diffs'].apply(_check_category)].copy()\n",
                "\n",
                "    # data_dfからトークンの差が以下の文章のみを抜き出す\n",
                "    data_df = data_df[data_df.apply(check_token_count, axis=1)].copy()\n",
                "\n",
                "    # 文章の正規化\n",
                "    data_df.pre_text = data_df.pre_text.apply(_normalize)\n",
                "    data_df.post_text = data_df.post_text.apply(_normalize)\n",
                "\n",
                "    return data_df[['pre_text', 'post_text']].to_dict(orient='records')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# データセットを読み込んで加工する\n",
                "dir_path = \"data/chapter9/jwtd_v2.0/\"\n",
                "train_df = pd.read_json(dir_path+\"train.jsonl\", orient='records', lines=True)\n",
                "test_df = pd.read_json(dir_path+\"test.jsonl\", orient='records', lines=True)\n",
                "\n",
                "# train/val\n",
                "tmp_dataset = create_dataset(train_df)\n",
                "random.shuffle(tmp_dataset)\n",
                "train_size = int(len(tmp_dataset) * 0.8)\n",
                "train_dataset = tmp_dataset[:train_size]\n",
                "val_dataset = tmp_dataset[train_size:]\n",
                "# test\n",
                "test_dataset = create_dataset(test_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9.5 ファインチューニング"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ファインチューニングのためのデータローダを定義する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-14\n",
                "\n",
                "def create_dataset_for_loader(\n",
                "        tokenizer,\n",
                "        dataset: dict,\n",
                "        max_length: int\n",
                "    ):\n",
                "    \"\"\"\n",
                "    データセットをデータローダに入力可能な形式にする\n",
                "    \"\"\"\n",
                "    dataset_for_loader = []\n",
                "    for sample in tqdm(dataset):\n",
                "        pre_text = sample['pre_text']\n",
                "        post_text = sample['post_text']\n",
                "        encoding = tokenizer.encode_plus_tagged(\n",
                "            pre_text, post_text, max_length=max_length\n",
                "        )\n",
                "        encoding = {key: torch.tensor(value) for key, value in encoding.items()}\n",
                "        dataset_for_loader.append(encoding)\n",
                "    return dataset_for_loader\n",
                "\n",
                "\n",
                "max_length = 32\n",
                "train_dataset_for_loader = create_dataset_for_loader(\n",
                "    tokenizer, train_dataset, max_length\n",
                ")\n",
                "val_dataset_for_loader = create_dataset_for_loader(\n",
                "    tokenizer, val_dataset, max_length\n",
                ")\n",
                "\n",
                "# データローダの作成\n",
                "train_dataloader = DataLoader(train_dataset_for_loader, batch_size=32, shuffle=True)\n",
                "val_dataloader = DataLoader(val_dataset_for_loader, batch_size=256)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PyTorch Lightningでファインチューニングを行う。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-15\n",
                "\n",
                "class BertForMaskedLM_pl(pl.LightningModule):\n",
                "    def __init__(self, model_name, lr):\n",
                "        super().__init__()\n",
                "        self.save_hyperparameters()\n",
                "        self.model = BertForMaskedLM.from_pretrained(model_name)\n",
                "    \n",
                "    def training_step(self, batch, batch_idx):\n",
                "        output = self.model(**batch)\n",
                "        train_loss = output.loss\n",
                "        self.log('train_loss', train_loss)\n",
                "        return train_loss\n",
                "\n",
                "    def validation_step(self, batch, batch_idx):\n",
                "        output = self.model(**batch)\n",
                "        val_loss = output.loss\n",
                "        self.log('val_loss', val_loss)\n",
                "    \n",
                "    def configure_optimizers(self):\n",
                "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "checkpoint = pl.callbacks.ModelCheckpoint(\n",
                "    monitor='val_loss',\n",
                "    mode='min',\n",
                "    save_top_k=1,\n",
                "    save_weights_only=True,\n",
                "    dirpath=\"model/\",\n",
                ")\n",
                "\n",
                "# GPUで動作させる場合、accelerator引数に'gpu'を渡す。\n",
                "# 2枚以上のGPUで分散処理(DistributedDataParallel)する場合、\n",
                "# devices引数に>1のintを、\n",
                "# strategy引数に'ddp'(.py)や'ddp_notebook'(.ipynb)文字列を渡す\n",
                "# Pytorchのプロセスtorch.cuda()がGPUメモリに残っていると動かないので注意\n",
                "trainer = pl.Trainer(\n",
                "    accelerator='gpu',\n",
                "    devices=1, # GPUが2枚だとなぜかエラーが出る\n",
                "    # strategy='ddp_notebook', # DistributedDataParallel\n",
                "    max_epochs=5,\n",
                "    callbacks=[checkpoint],\n",
                ")\n",
                "\n",
                "# ファインチューニング\n",
                "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
                "model = BertForMaskedLM_pl(model_name, lr=1e-5)\n",
                "trainer.fit(model, train_dataloader, val_dataloader)\n",
                "best_model_path = checkpoint.best_model_path"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9.6 性能評価"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-16\n",
                "\n",
                "def predict(text, tokenizer, model):\n",
                "    \"\"\"\n",
                "    文章からBERTが予測した文章を出力\n",
                "    \"\"\"\n",
                "    # 受け取った文章を符号化する\n",
                "    encoding, spans = tokenizer.encode_plus_untagged(\n",
                "        text, return_tensors='pt'\n",
                "    )\n",
                "    encoding = {key: value.cuda() for key, value in encoding.items()}\n",
                "\n",
                "    # \n",
                "    with torch.no_grad():\n",
                "        output = model(**encoding)\n",
                "    scores = output.logits\n",
                "    labels_pred = scores[0].argmax(axis=1).cpu().numpy().tolist()\n",
                "\n",
                "    text_pred = tokenizer.convert_output_to_text(\n",
                "        text, labels_pred, spans\n",
                "    )\n",
                "    return text_pred"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "適当な文章に対してBERTによる文章校正を行ってみる"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_lst = [\n",
                "    'ユーザーの試行に合わせた楽曲を配信する。',\n",
                "    'メールに明日の会議の史料を添付した。',\n",
                "    '乳酸菌で牛乳を発行するとヨーグルトができる。',\n",
                "    '突然、子供が帰省を発した。'\n",
                "]\n",
                "\n",
                "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
                "tokenizer = SC_tokenizer.from_pretrained(model_name)\n",
                "model = BertForMaskedLM_pl.load_from_checkpoint(best_model_path)\n",
                "model = model.model.cuda()\n",
                "\n",
                "for text in text_lst:\n",
                "    text_pred = predict(text, tokenizer, model)\n",
                "    print()\n",
                "    print(f\"入力: {text}\")\n",
                "    print(f\"出力: {text_pred}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "次に`test_dataset`の評価を行う"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "correct_num = 0\n",
                "for sample in tqdm(test_dataset):\n",
                "    pre_text = sample['pre_text']\n",
                "    post_text = sample['post_text']\n",
                "    text_pred = predict(pre_text, tokenizer, model)\n",
                "\n",
                "    if post_text == text_pred:\n",
                "        correct_num += 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Accuracy: {correct_num / len(test_dataset):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "誤変換の単語を特定した割合を評価する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-18\n",
                "\n",
                "correct_position_num = 0\n",
                "for sample in tqdm(test_dataset):\n",
                "    pre_text = sample['pre_text']\n",
                "    post_text = sample['post_text']\n",
                "\n",
                "    # 符号化\n",
                "    pre_encoding = tokenizer(pre_text)\n",
                "    post_encoding = tokenizer(post_text)\n",
                "    pre_input_ids = pre_encoding['input_ids'] # 誤変換していた文章の符号列\n",
                "    post_input_ids = post_encoding['input_ids'] # 正解文の符号列\n",
                "\n",
                "    pre_encoding = {key: torch.tensor([value]).cuda() for key, value in pre_encoding.items()}\n",
                "\n",
                "    # 予測\n",
                "    with torch.no_grad():\n",
                "        output = model(**pre_encoding)\n",
                "    scores = output.logits\n",
                "    pred_input_ids = scores[0].argmax(axis=1).cpu().numpy().tolist()\n",
                "\n",
                "    # 特殊トークンを取り除く\n",
                "    pre_input_ids = pre_input_ids[1:-1]\n",
                "    post_input_ids = post_input_ids[1:-1]\n",
                "    pred_input_ids = pred_input_ids[1:-1]\n",
                "\n",
                "    # 誤変換した漢字を特定できているか、符号列を比較して判定\n",
                "    detect_flag = True\n",
                "    for pre_token, post_token, pred_token in zip(pre_input_ids, post_input_ids, pred_input_ids):\n",
                "        # 正しいトークンの場合\n",
                "        if pre_token == post_token:\n",
                "            # 正しいトークンなのに誤って別のトークンに変換している場合\n",
                "            if pre_token != pred_token:\n",
                "                detect_flag = False\n",
                "                break\n",
                "        else:\n",
                "            # 誤変換のトークンをそのままにしている場合\n",
                "            if pre_token == pred_token:\n",
                "                detect_flag = False\n",
                "                break\n",
                "    if detect_flag:\n",
                "        correct_position_num += 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Accuracy: {correct_position_num / len(test_dataset):.2f}\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "Chapter9.ipynb",
            "provenance": [
                {
                    "file_id": "https://github.com/stockmarkteam/bert-book/blob/master/Chapter9.ipynb",
                    "timestamp": 1630577154754
                }
            ]
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
