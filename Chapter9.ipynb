{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "OQ4zTAe6s78f"
            },
            "source": [
                "# 9章\n",
                "\n",
                "- 本章で用いる「[日本語Wikipedia入力誤りデータセット](https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9EWikipedia%E5%85%A5%E5%8A%9B%E8%AA%A4%E3%82%8A%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88)」は現在バージョン2が公開されていますが、本章では[バージョン1](https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9EWikipedia%E5%85%A5%E5%8A%9B%E8%AA%A4%E3%82%8A%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88v1)を用いています。"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "本章では、文章校正タスクのうち漢字の誤変換タスクを扱う。BERTでは、文章校正をトークンの分類問題として扱うことで実装できる。<br>\n",
                "正しいトークンについては同じトークンを、間違っていると考えられるトークンにはBERTの語彙の中から正しいと予測したトークンを返す。\n",
                "\n",
                "|誤変換|正しい文章|\n",
                "|:-|:-|\n",
                "|優勝|優勝|\n",
                "|トロフィー|トロフィー|\n",
                "|を|を|\n",
                "|変換|返還|\n",
                "|し|し|\n",
                "|た|た|\n",
                "|。|。|\n",
                "\n",
                "一方で、以下のようにこの方法では扱えない文章が存在する。\n",
                "\n",
                "|誤変換|正しい文章|\n",
                "|:-|:-|\n",
                "|投|当初|\n",
                "|##書|は|\n",
                "|は|、|\n",
                "|、|実行|\n",
                "|実行|を|\n",
                "\n",
                "本章では、上記のような文章を取り扱わないこととする。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-3\n",
                "\n",
                "import os\n",
                "import random\n",
                "import unicodedata\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import pprint\n",
                "import math\n",
                "\n",
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
                "import pytorch_lightning as pl"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "トークナイザを定義する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 90,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-4\n",
                "\n",
                "class SC_tokenizer(BertJapaneseTokenizer):\n",
                "    def encode_plus_tagged(\n",
                "        self,\n",
                "        wrong_text: str,\n",
                "        correct_text: str,\n",
                "        max_length: int = 128,\n",
                "    ) -> dict:\n",
                "        \"\"\"\n",
                "        ファインチューニング用\n",
                "        誤変換を含む文章と正しい文章を渡し符号化し、\n",
                "        誤変換文章のlabelsを正しい文章のinput_idsに置き換える\n",
                "        \"\"\"\n",
                "        # 文章から直接符号化\n",
                "        # tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)で\n",
                "        # インスタンス化したtokinzer(text)関数と同等\n",
                "        wrong_encoding = self(\n",
                "            wrong_text, max_length=max_length, padding='max_length', truncation=True\n",
                "        )\n",
                "        correct_encoding = self(\n",
                "            correct_text, max_length=max_length, padding='max_length', truncation=True\n",
                "        )\n",
                "        # 正しい文章の符号をラベルとする\n",
                "        wrong_encoding['labels'] = correct_encoding['input_ids']\n",
                "\n",
                "        return wrong_encoding\n",
                "    \n",
                "    def encode_plus_untagged(\n",
                "        self,\n",
                "        text: str,\n",
                "        max_length: int = None,\n",
                "        return_tensors: str = 'pt',\n",
                "    ) -> (dict, list):\n",
                "        \"\"\"\n",
                "        文章をトークン化し、それぞれのトークンと文章中の文字列を対応付ける\n",
                "        推論時にトークンごとのラベルを予測し、最終的に固有表現に変換する\n",
                "        未知語や文章中の空白(MeCabにより消去される)に対しての処理が必要となる\n",
                "        そのため、各トークンが元の文章のどの位置にあったかを特定しておく\n",
                "        \"\"\"\n",
                "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
                "        tokens = []\n",
                "        tokens_original = []\n",
                "        # 単語をサブワードに分割してlistに格納\n",
                "        for word in words:\n",
                "            tokens_subword = self.subword_tokenizer.tokenize(word)\n",
                "            tokens.extend(tokens_subword)\n",
                "            # 未知語対応\n",
                "            if tokens_subword[0] == '[UNK]':\n",
                "                tokens_original.append(word)\n",
                "            else:\n",
                "                tokens_original.extend([token.replace(\"##\", \"\") for token in tokens_subword])\n",
                "        \n",
                "        # トークンが文章中のどの位置にあるかを走査する\n",
                "        position = 0\n",
                "        spans = []\n",
                "        for token in tokens_original:\n",
                "            length = len(token)\n",
                "            while True:\n",
                "                if token != text[position: position + length]:\n",
                "                    position += 1\n",
                "                else:\n",
                "                    spans.append([position, position + length])\n",
                "                    position += length\n",
                "                    break\n",
                "        \n",
                "        # トークンをID化する\n",
                "        input_ids = self.convert_tokens_to_ids(tokens)\n",
                "        # トークンIDを符号化する\n",
                "        encoding = self.prepare_for_model(\n",
                "            input_ids, \n",
                "            max_length=max_length,\n",
                "            padding = 'max_length' if max_length else False,\n",
                "            truncation = True if max_length else False,\n",
                "        )\n",
                "        sequence_length = len(encoding['input_ids']) # 符号化した文章の長さ\n",
                "        # 先頭トークン[CLS]用のspanを追加する\n",
                "        # このとき、次の[SEP]トークンを一緒に削除しておく\n",
                "        spans = [[-1, -1]] + spans[:sequence_length - 2]\n",
                "        # 末尾トークン[SEP]、末尾の空トークン[PAD]用のspanを追加する\n",
                "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
                "\n",
                "        # 引数に応じてtorch.Tensor型に返還\n",
                "        # 次元を追加する必要がある\n",
                "        if return_tensors == 'pt':\n",
                "            encoding = {key: torch.tensor([value]) for key, value in encoding.items()}\n",
                "        \n",
                "        return (encoding, spans)\n",
                "    \n",
                "    def convert_output_to_text(\n",
                "        self, \n",
                "        text: str,\n",
                "        labels_arg: list,\n",
                "        spans_arg: list,\n",
                "    ) -> str:\n",
                "        \"\"\"\n",
                "        文章、各トークンのラベルの予測値、文章中での位置から\n",
                "        予測された文章に変換する\n",
                "        \"\"\"\n",
                "        # 文章の長さチェック\n",
                "        assert len(labels_arg) == len(spans_arg)\n",
                "\n",
                "        # 特殊トークンを取り除く\n",
                "        labels = []\n",
                "        spans = []\n",
                "        for label, span in zip(labels_arg, spans_arg):\n",
                "            if span[0] != -1:\n",
                "                labels.append(label)\n",
                "                spans.append(span)\n",
                "        \n",
                "        # モデルが予測した文章を生成する\n",
                "        text_pred = \"\"\n",
                "        position = 0\n",
                "        for label, span in zip(labels, spans):\n",
                "            start, end = span\n",
                "            # 空白文字の処理\n",
                "            if position != start:\n",
                "                text_pred += text[position: start]\n",
                "            token_pred = self.convert_ids_to_tokens(label) # labelをトークン化\n",
                "            token_pred = token_pred.replace(\"##\", \"\") # サブワードの##を削除\n",
                "            token_pred = unicodedata.normalize('NFKC', token_pred) # 文字列の正規化\n",
                "            text_pred += token_pred\n",
                "            position = end\n",
                "        return text_pred"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "定義したトークナイザの動きを確認する。\n",
                "\n",
                "モデルとトークナイザの呼び出し"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "metadata": {
                "id": "HBTlINuoGXNc"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
                        "The class this function is called from is 'SC_tokenizer'.\n"
                    ]
                }
            ],
            "source": [
                "# 9-5\n",
                "\n",
                "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
                "tokenizer = SC_tokenizer.from_pretrained(model_name)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`encode_plus_tagged()`メソッドは、誤変換した文章と正しい文章が渡されたら、誤変換した文章を符号化し`labels`に正しい文章のトークンIDを付与する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 92,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-6\n",
                "\n",
                "wrong_text = \"優勝トロフィーを変換した\"\n",
                "correct_text = \"優勝トロフィーを返還した\"\n",
                "\n",
                "encoding = tokenizer.encode_plus_tagged(\n",
                "    wrong_text, correct_text, max_length=12,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 93,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
                        " 'input_ids': [2, 759, 18204, 11, 4618, 15, 10, 3, 0, 0, 0, 0],\n",
                        " 'labels': [2, 759, 18204, 11, 8274, 15, 10, 3, 0, 0, 0, 0],\n",
                        " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
                    ]
                }
            ],
            "source": [
                "pprint.pprint(encoding)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`encode_plus_untagged()`メソッドは、文章を符号化し、空白や未知語を考慮した上でそれぞれのトークンの位置を返す。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 94,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-7\n",
                "\n",
                "wrong_text = \"優勝トロフィーを変換した\"\n",
                "encoding, spans = tokenizer.encode_plus_untagged(\n",
                "    wrong_text, return_tensors='pt'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 95,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'input_ids': tensor([[    2,   759, 18204,    11,  4618,    15,    10,     3]]),\n",
                            " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]),\n",
                            " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
                        ]
                    },
                    "execution_count": 95,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 96,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[[-1, -1], [0, 2], [2, 7], [7, 8], [8, 10], [10, 11], [11, 12], [-1, -1]]"
                        ]
                    },
                    "execution_count": 96,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "spans"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`convert_output_to_text()`関数は、文章とラベル列、各トークンの文章中の位置から予測した文章を出力する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 97,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-8\n",
                "\n",
                "labels_pred = [2, 759, 18204, 11, 8274, 15, 10, 3]\n",
                "text_pred = tokenizer.convert_output_to_text(\n",
                "    wrong_text, labels_pred, spans\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 98,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'優勝トロフィーを返還した'"
                        ]
                    },
                    "execution_count": 98,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "text_pred"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9.2 BERTにおける実装"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`transformers.BertForMaskedLM`クラスを用いる。このクラスは各トークンに対してその市に入るトークンを語彙の中から選ぶもので、これは`transformers.BertForTokenClassification`でラベル数を語彙数としたものと入出力関係が同じと考えることができる。<br>\n",
                "一方で、`transformers.BertForMaskedLM`は、事前学習でランダムに選ばれたトークンを`[MASK]`もしくはランダムなトークンに置き換え、またはそのままで、元のトークンが何であったかを予測するという学習が行われる。これは、文章校正タスクと類似しており、分類器の初期パラメータとして事前学習により得られたものを用いる`transformers.BertForMaskedLM`のほうが、`transformers.BertForTokenClassification`と比較してある程度妥当なパラメータを備えていることが期待されるためである。これにより、ファインチューニングの学習時間の短縮が期待できる。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 99,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
                        "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                }
            ],
            "source": [
                "# 9-9\n",
                "\n",
                "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
                "model = BertForMaskedLM.from_pretrained(model_name)\n",
                "model = model.cuda()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "文章の符号化から予測された文章の出力までは、以下のように実装する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 100,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-10\n",
                "\n",
                "text = \"優勝トロフィーを変換した。\"\n",
                "\n",
                "# 符号化およびトークンの文章中の位置を取得\n",
                "encoding, spans = tokenizer.encode_plus_untagged(\n",
                "    text, return_tensors='pt'\n",
                ")\n",
                "encoding = {key: value.cuda() for key, value in encoding.items()}\n",
                "\n",
                "# BERTに入力\n",
                "with torch.no_grad():\n",
                "    output = model(**encoding)\n",
                "scores = output.logits\n",
                "labels_pred = scores[0].argmax(axis=1).cpu().numpy().tolist()\n",
                "\n",
                "text_pred = tokenizer.convert_output_to_text(\n",
                "    text, labels_pred, spans\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 101,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'優勝トロフィーを獲得した。'"
                        ]
                    },
                    "execution_count": 101,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "text_pred"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "次に、誤変換した文章と正しい文章をBERTに入力して損失を計算する。入力時に、それぞれの文章を符号化し、正しい文章についてはラベル列を正とする。`tokenizer.encode_plus_tagged()`を用いることで、ラベル列を含んだデータを作成できる。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 102,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-11\n",
                "\n",
                "data = [\n",
                "    {\n",
                "        'wrong_text': '優勝トロフィーを変換した。',\n",
                "        'correct_text': '優勝トロフィーを返還した。',\n",
                "    },\n",
                "    {\n",
                "        'wrong_text': '人と森は強制している。',\n",
                "        'correct_text': '人と森は共生している。',\n",
                "    }\n",
                "]\n",
                "\n",
                "# データの符号化\n",
                "max_length = 32\n",
                "dataset_for_loader = []\n",
                "for sample in data:\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    encoding = tokenizer.encode_plus_tagged(\n",
                "        wrong_text, correct_text, max_length=max_length\n",
                "    )\n",
                "    encoding = {key: torch.tensor(value) for key, value in encoding.items()}\n",
                "    dataset_for_loader.append(encoding)\n",
                "\n",
                "dataloader = DataLoader(dataset_for_loader, batch_size=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 103,
            "metadata": {},
            "outputs": [],
            "source": [
                "for batch in dataloader:\n",
                "    encoding = {key: value.cuda() for key, value in batch.items()}\n",
                "    output = model(**encoding)\n",
                "    loss = output.loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 104,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor(13.2660, device='cuda:0', grad_fn=<NllLossBackward0>)"
                        ]
                    },
                    "execution_count": 104,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "loss"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9.4 日本語Wikipedia誤りデータセット\n",
                "\n",
                "本章では、京都大学の言語メディア研究室が作成した[日本語Wikipedia誤りデータセット](\"https://nlp.ist.i.kyoto-u.ac.jp/?日本語Wikipedia入力誤りデータセット\")を用いて学習を行う。このデータセットは、日本語Wikipediaの差分から間違った文章と正しい文章のペアを抽出している。\n",
                "\n",
                "まずは、データセット(v2.0)をダウンロードする。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 105,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-2\n",
                "\n",
                "os.makedirs(\"data/chapter9\", exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 110,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
                        "                                 Dload  Upload   Total   Spent    Left  Speed\n",
                        "100   249  100   249    0     0   2480      0 --:--:-- --:--:-- --:--:--  2490\n",
                        " 17 94.0M   17 16.0M    0     0  1905k      0  0:00:50  0:00:08  0:00:42 1890k"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "100 94.0M  100 94.0M    0     0  1823k      0  0:00:52  0:00:52 --:--:-- 2291k\n"
                    ]
                }
            ],
            "source": [
                "# 9-12\n",
                "\n",
                "!curl -o \"data/chapter9/jwtd.tar.gz\" \"https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd_v2.0.tar.gz&name=JWTDv2.0.tar.gz\"\n",
                "!cd \"data/chapter9\" && tar -zxf \"jwtd.tar.gz\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "本データセットはJSON形式で、`pre_text`に修正前の文章、`post_text`に修正後の文章が格納されている。\n",
                "さらに、`diffs`配列の中の`category`に入力誤りの種類(誤字、脱字など)が、`pre_str`が修正前、`post_str`が修正後の単語が格納されている。\n",
                "\n",
                "本章では漢字誤変換のみを扱う。`category`が`kanji-conversion_a`となっているもののみを利用する。\n",
                "\n",
                "まずデータを読み込む。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 107,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
                        "The class this function is called from is 'SC_tokenizer'.\n"
                    ]
                }
            ],
            "source": [
                "# 9-13\n",
                "\n",
                "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
                "tokenizer = SC_tokenizer.from_pretrained(model_name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 112,
            "metadata": {},
            "outputs": [],
            "source": [
                "category_type = 'kanji-conversion_a'\n",
                "\n",
                "def create_dataset(data_df: pd.DataFrame) -> dict:\n",
                "    def _check_category(lst: list) -> bool:\n",
                "        \"\"\"\n",
                "        data_df.diffsを走査して、\n",
                "        - listの長さが2以下\n",
                "        - すべてのcategoryがkanji-conversion_a\n",
                "        のすべてを満たすときにTrue、そうでないときにFalseを返す\n",
                "        \"\"\"\n",
                "        checked_lst = [dct['category'] == category_type for dct in lst]\n",
                "        category_bool =  math.prod(checked_lst) # リスト内を掛け算してFalseがあれば0が返る\n",
                "        length_bool = len(checked_lst) <= 2\n",
                "        return bool(category_bool * length_bool)\n",
                "\n",
                "    def _normalize(text: str) -> str:\n",
                "        \"\"\"\n",
                "        文字列の正規化を行う\n",
                "        \"\"\"\n",
                "        text = text.strip() # 改行文字や全角スペースを取り除く\n",
                "        return unicodedata.normalize('NFKC', text) # NFKCで正規化\n",
                "\n",
                "    # data_df.diffsのtypeをobject -> listに変換\n",
                "    data_df.diffs = data_df.diffs.apply(lambda x: list(x))\n",
                "\n",
                "    # data_dfからcategoryがkanji-conversion_aかつ誤変換が2以下の文章のみを抜き出す\n",
                "    data_df = data_df[data_df['diffs'].apply(_check_category)].copy()\n",
                "\n",
                "    # 文章の正規化\n",
                "    data_df.pre_text = data_df.pre_text.apply(_normalize)\n",
                "    data_df.post_text = data_df.post_text.apply(_normalize)\n",
                "\n",
                "    return data_df[['pre_text', 'post_text']].to_dict(orient='records')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 113,
            "metadata": {},
            "outputs": [],
            "source": [
                "# データセットを読み込んで加工する\n",
                "dir_path = \"data/chapter9/jwtd_v2.0/\"\n",
                "train_df = pd.read_json(dir_path+\"train.jsonl\", orient='records', lines=True)\n",
                "test_df = pd.read_json(dir_path+\"test.jsonl\", orient='records', lines=True)\n",
                "\n",
                "# train/val\n",
                "tmp_dataset = create_dataset(train_df)\n",
                "random.shuffle(tmp_dataset)\n",
                "train_size = int(len(tmp_dataset) * 0.8)\n",
                "train_dataset = tmp_dataset[:train_size]\n",
                "val_dataset = tmp_dataset[train_size:]\n",
                "# test\n",
                "test_dataset = create_dataset(test_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9.5 ファインチューニング"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ファインチューニングのためのデータローダを定義する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "hlr9SXDNGXNh"
            },
            "outputs": [],
            "source": [
                "# 9-14\n",
                "def create_dataset_for_loader(tokenizer, dataset, max_length):\n",
                "    \"\"\"\n",
                "    データセットをデータローダに入力可能な形式にする。\n",
                "    \"\"\"\n",
                "    dataset_for_loader = []\n",
                "    for sample in tqdm(dataset):\n",
                "        wrong_text = sample['wrong_text']\n",
                "        correct_text = sample['correct_text']\n",
                "        encoding = tokenizer.encode_plus_tagged(\n",
                "            wrong_text, correct_text, max_length=max_length\n",
                "        )\n",
                "        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
                "        dataset_for_loader.append(encoding)\n",
                "    return dataset_for_loader\n",
                "\n",
                "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# データセットの作成\n",
                "max_length = 32\n",
                "dataset_train_for_loader = create_dataset_for_loader(\n",
                "    tokenizer, dataset_train, max_length\n",
                ")\n",
                "dataset_val_for_loader = create_dataset_for_loader(\n",
                "    tokenizer, dataset_val, max_length\n",
                ")\n",
                "\n",
                "# データローダの作成\n",
                "dataloader_train = DataLoader(\n",
                "    dataset_train_for_loader, batch_size=32, shuffle=True\n",
                ")\n",
                "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "c-j7R6z0GXNh"
            },
            "outputs": [],
            "source": [
                "# 9-15\n",
                "class BertForMaskedLM_pl(pl.LightningModule):\n",
                "        \n",
                "    def __init__(self, model_name, lr):\n",
                "        super().__init__()\n",
                "        self.save_hyperparameters()\n",
                "        self.bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
                "        \n",
                "    def training_step(self, batch, batch_idx):\n",
                "        output = self.bert_mlm(**batch)\n",
                "        loss = output.loss\n",
                "        self.log('train_loss', loss)\n",
                "        return loss\n",
                "        \n",
                "    def validation_step(self, batch, batch_idx):\n",
                "        output = self.bert_mlm(**batch)\n",
                "        val_loss = output.loss\n",
                "        self.log('val_loss', val_loss)\n",
                "   \n",
                "    def configure_optimizers(self):\n",
                "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
                "\n",
                "checkpoint = pl.callbacks.ModelCheckpoint(\n",
                "    monitor='val_loss',\n",
                "    mode='min',\n",
                "    save_top_k=1,\n",
                "    save_weights_only=True,\n",
                "    dirpath='model/'\n",
                ")\n",
                "\n",
                "trainer = pl.Trainer(\n",
                "    gpus=1,\n",
                "    max_epochs=5,\n",
                "    callbacks=[checkpoint]\n",
                ")\n",
                "\n",
                "# ファインチューニング\n",
                "model = BertForMaskedLM_pl(MODEL_NAME, lr=1e-5)\n",
                "trainer.fit(model, dataloader_train, dataloader_val)\n",
                "best_model_path = checkpoint.best_model_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "w2eNTz6OsvVP"
            },
            "outputs": [],
            "source": [
                "# 9-16\n",
                "def predict(text, tokenizer, bert_mlm):\n",
                "    \"\"\"\n",
                "    文章を入力として受け、BERTが予測した文章を出力\n",
                "    \"\"\"\n",
                "    # 符号化\n",
                "    encoding, spans = tokenizer.encode_plus_untagged(\n",
                "        text, return_tensors='pt'\n",
                "    ) \n",
                "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
                "\n",
                "    # ラベルの予測値の計算\n",
                "    with torch.no_grad():\n",
                "        output = bert_mlm(**encoding)\n",
                "        scores = output.logits\n",
                "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
                "\n",
                "    # ラベル列を文章に変換\n",
                "    predict_text = tokenizer.convert_bert_output_to_text(\n",
                "        text, labels_predicted, spans\n",
                "    )\n",
                "\n",
                "    return predict_text\n",
                "\n",
                "# いくつかの例に対してBERTによる文章校正を行ってみる。\n",
                "text_list = [\n",
                "    'ユーザーの試行に合わせた楽曲を配信する。',\n",
                "    'メールに明日の会議の史料を添付した。',\n",
                "    '乳酸菌で牛乳を発行するとヨーグルトができる。',\n",
                "    '突然、子供が帰省を発した。'\n",
                "]\n",
                "\n",
                "# トークナイザ、ファインチューニング済みのモデルのロード\n",
                "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
                "model = BertForMaskedLM_pl.load_from_checkpoint(best_model_path)\n",
                "bert_mlm = model.bert_mlm.cuda()\n",
                "\n",
                "for text in text_list:\n",
                "    predict_text = predict(text, tokenizer, bert_mlm) # BERTによる予測\n",
                "    print('---')\n",
                "    print(f'入力：{text}')\n",
                "    print(f'出力：{predict_text}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "0zwqdG4SGXNi",
                "jupyter": {
                    "outputs_hidden": false
                }
            },
            "outputs": [],
            "source": [
                "# 9-17\n",
                "# BERTで予測を行い、正解数を数える。\n",
                "correct_num = 0 \n",
                "for sample in tqdm(dataset_test):\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    predict_text = predict(wrong_text, tokenizer, bert_mlm) # BERT予測\n",
                "   \n",
                "    if correct_text == predict_text: # 正解の場合\n",
                "        correct_num += 1\n",
                "\n",
                "print(f'Accuracy: {correct_num/len(dataset_test):.2f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ZUj8v6dPGXNj"
            },
            "outputs": [],
            "source": [
                "# 9-18\n",
                "correct_position_num = 0 # 正しく誤変換の漢字を特定できたデータの数\n",
                "for sample in tqdm(dataset_test):\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    \n",
                "    # 符号化\n",
                "    encoding = tokenizer(wrong_text)\n",
                "    wrong_input_ids = encoding['input_ids'] # 誤変換の文の符合列\n",
                "    encoding = {k: torch.tensor([v]).cuda() for k,v in encoding.items()}\n",
                "    correct_encoding = tokenizer(correct_text)\n",
                "    correct_input_ids = correct_encoding['input_ids'] # 正しい文の符合列\n",
                "    \n",
                "    # 文章を予測\n",
                "    with torch.no_grad():\n",
                "        output = bert_mlm(**encoding)\n",
                "        scores = output.logits\n",
                "        # 予測された文章のトークンのID\n",
                "        predict_input_ids = scores[0].argmax(-1).cpu().numpy().tolist() \n",
                "\n",
                "    # 特殊トークンを取り除く\n",
                "    wrong_input_ids = wrong_input_ids[1:-1]\n",
                "    correct_input_ids =  correct_input_ids[1:-1]\n",
                "    predict_input_ids =  predict_input_ids[1:-1]\n",
                "    \n",
                "    # 誤変換した漢字を特定できているかを判定\n",
                "    # 符合列を比較する。\n",
                "    detect_flag = True\n",
                "    for wrong_token, correct_token, predict_token \\\n",
                "        in zip(wrong_input_ids, correct_input_ids, predict_input_ids):\n",
                "\n",
                "        if wrong_token == correct_token: # 正しいトークン\n",
                "            # 正しいトークンなのに誤って別のトークンに変換している場合\n",
                "            if wrong_token != predict_token: \n",
                "                detect_flag = False\n",
                "                break\n",
                "        else: # 誤変換のトークン\n",
                "            # 誤変換のトークンなのに、そのままにしている場合\n",
                "            if wrong_token == predict_token: \n",
                "                detect_flag = False\n",
                "                break\n",
                "\n",
                "    if detect_flag: # 誤変換の漢字の位置を正しく特定できた場合\n",
                "        correct_position_num += 1\n",
                "        \n",
                "print(f'Accuracy: {correct_position_num/len(dataset_test):.2f}')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "Chapter9.ipynb",
            "provenance": [
                {
                    "file_id": "https://github.com/stockmarkteam/bert-book/blob/master/Chapter9.ipynb",
                    "timestamp": 1630577154754
                }
            ]
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
