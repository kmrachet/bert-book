{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "OQ4zTAe6s78f"
            },
            "source": [
                "# 9章\n",
                "\n",
                "- 本章で用いる「[日本語Wikipedia入力誤りデータセット](https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9EWikipedia%E5%85%A5%E5%8A%9B%E8%AA%A4%E3%82%8A%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88)」は現在バージョン2が公開されていますが、本章では[バージョン1](https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9EWikipedia%E5%85%A5%E5%8A%9B%E8%AA%A4%E3%82%8A%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88v1)を用いています。"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "本章では、文章校正タスクのうち漢字の誤変換タスクを扱う。BERTでは、文章校正をトークンの分類問題として扱うことで実装できる。<br>\n",
                "正しいトークンについては同じトークンを、間違っていると考えられるトークンにはBERTの語彙の中から正しいと予測したトークンを返す。\n",
                "\n",
                "|誤変換|正しい文章|\n",
                "|:-|:-|\n",
                "|優勝|優勝|\n",
                "|トロフィー|トロフィー|\n",
                "|を|を|\n",
                "|変換|返還|\n",
                "|し|し|\n",
                "|た|た|\n",
                "|。|。|\n",
                "\n",
                "一方で、以下のようにこの方法では扱えない文章が存在する。\n",
                "\n",
                "|誤変換|正しい文章|\n",
                "|:-|:-|\n",
                "|投|当初|\n",
                "|##書|は|\n",
                "|は|、|\n",
                "|、|実行|\n",
                "|実行|を|\n",
                "\n",
                "本章では、上記のような文章を取り扱わないこととする。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-3\n",
                "\n",
                "import os\n",
                "import random\n",
                "import unicodedata\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import pprint\n",
                "\n",
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
                "import pytorch_lightning as pl"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "トークナイザを定義する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-4\n",
                "\n",
                "class SC_tokenizer(BertJapaneseTokenizer):\n",
                "    def encode_plus_tagged(\n",
                "        self,\n",
                "        wrong_text: str,\n",
                "        correct_text: str,\n",
                "        max_length: int = 128,\n",
                "    ) -> dict:\n",
                "        \"\"\"\n",
                "        ファインチューニング用\n",
                "        誤変換を含む文章と正しい文章を渡し符号化し、\n",
                "        誤変換文章のlabelsを正しい文章のinput_idsに置き換える\n",
                "        \"\"\"\n",
                "        # 文章から直接符号化\n",
                "        # tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)で\n",
                "        # インスタンス化したtokinzer(text)関数と同等\n",
                "        wrong_encoding = self(\n",
                "            wrong_text, max_length=max_length, padding='max_length', truncation=True\n",
                "        )\n",
                "        correct_encoding = self(\n",
                "            correct_text, max_length=max_length, padding='max_length', truncation=True\n",
                "        )\n",
                "        # 正しい文章の符号をラベルとする\n",
                "        wrong_encoding['labels'] = correct_encoding['input_ids']\n",
                "\n",
                "        return wrong_encoding\n",
                "    \n",
                "    def encode_plus_untagged(\n",
                "        self,\n",
                "        text: str,\n",
                "        max_length: int = None,\n",
                "        return_tensors: str = 'pt',\n",
                "    ) -> (dict, list):\n",
                "        \"\"\"\n",
                "        文章をトークン化し、それぞれのトークンと文章中の文字列を対応付ける\n",
                "        推論時にトークンごとのラベルを予測し、最終的に固有表現に変換する\n",
                "        未知語や文章中の空白(MeCabにより消去される)に対しての処理が必要となる\n",
                "        そのため、各トークンが元の文章のどの位置にあったかを特定しておく\n",
                "        \"\"\"\n",
                "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
                "        tokens = []\n",
                "        tokens_original = []\n",
                "        # 単語をサブワードに分割してlistに格納\n",
                "        for word in words:\n",
                "            tokens_subword = self.subword_tokenizer.tokenize(word)\n",
                "            tokens.extend(tokens_subword)\n",
                "            # 未知語対応\n",
                "            if tokens_subword[0] == '[UNK]':\n",
                "                tokens_original.append(word)\n",
                "            else:\n",
                "                tokens_original.extend([token.replace(\"##\", \"\") for token in tokens_subword])\n",
                "        \n",
                "        # トークンが文章中のどの位置にあるかを走査する\n",
                "        position = 0\n",
                "        spans = []\n",
                "        for token in tokens_original:\n",
                "            length = len(token)\n",
                "            while True:\n",
                "                if token != text[position: position + length]:\n",
                "                    position += 1\n",
                "                else:\n",
                "                    spans.append([position, position + length])\n",
                "                    position += length\n",
                "                    break\n",
                "        \n",
                "        # トークンをID化する\n",
                "        input_ids = self.convert_tokens_to_ids(tokens)\n",
                "        # トークンIDを符号化する\n",
                "        encoding = self.prepare_for_model(\n",
                "            input_ids, \n",
                "            max_length=max_length,\n",
                "            padding = 'max_length' if max_length else False,\n",
                "            truncation = True if max_length else False,\n",
                "        )\n",
                "        sequence_length = len(encoding['input_ids']) # 符号化した文章の長さ\n",
                "        # 先頭トークン[CLS]用のspanを追加する\n",
                "        # このとき、次の[SEP]トークンを一緒に削除しておく\n",
                "        spans = [[-1, -1]] + spans[:sequence_length - 2]\n",
                "        # 末尾トークン[SEP]、末尾の空トークン[PAD]用のspanを追加する\n",
                "        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n",
                "\n",
                "        # 引数に応じてtorch.Tensor型に返還\n",
                "        if return_tensors == 'pt':\n",
                "            encoding = {key: torch.tensor(value) for key, value in encoding.items()}\n",
                "        \n",
                "        return (encoding, spans)\n",
                "    \n",
                "    def convert_output_to_text(\n",
                "        self, \n",
                "        text: str,\n",
                "        labels_arg: list,\n",
                "        spans_arg: list,\n",
                "    ) -> str:\n",
                "        \"\"\"\n",
                "        文章、各トークンのラベルの予測値、文章中での位置から\n",
                "        予測された文章に変換する\n",
                "        \"\"\"\n",
                "        # 文章の長さチェック\n",
                "        assert len(labels_arg) == len(spans_arg)\n",
                "\n",
                "        # 特殊トークンを取り除く\n",
                "        labels = []\n",
                "        spans = []\n",
                "        for label, span in zip(labels_arg, spans_arg):\n",
                "            if span[0] != -1:\n",
                "                labels.append(label)\n",
                "                spans.append(span)\n",
                "        \n",
                "        # モデルが予測した文章を生成する\n",
                "        text_pred = \"\"\n",
                "        position = 0\n",
                "        for label, span in zip(labels, spans):\n",
                "            start, end = span\n",
                "            # 空白文字の処理\n",
                "            if position != start:\n",
                "                text_pred += text[position: start]\n",
                "            token_pred = self.convert_ids_to_tokens(label) # labelをトークン化\n",
                "            token_pred = token_pred.replace(\"##\", \"\") # サブワードの##を削除\n",
                "            token_pred = unicodedata.normalize('NFKC', token_pred) # 文字列の正規化\n",
                "            text_pred += token_pred\n",
                "            position = end\n",
                "        return text_pred"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "定義したトークナイザの動きを確認する。\n",
                "\n",
                "モデルとトークナイザの呼び出し"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "id": "HBTlINuoGXNc"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
                        "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
                        "The class this function is called from is 'SC_tokenizer'.\n"
                    ]
                }
            ],
            "source": [
                "# 9-5\n",
                "\n",
                "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
                "tokenizer = SC_tokenizer.from_pretrained(model_name)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`encode_plus_tagged()`メソッドは、誤変換した文章と正しい文章が渡されたら、誤変換した文章を符号化し`labels`に正しい文章のトークンIDを付与する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-6\n",
                "\n",
                "wrong_text = \"優勝トロフィーを変換した\"\n",
                "correct_text = \"優勝トロフィーを返還した\"\n",
                "\n",
                "encoding = tokenizer.encode_plus_tagged(\n",
                "    wrong_text, correct_text, max_length=12,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
                        " 'input_ids': [2, 759, 18204, 11, 4618, 15, 10, 3, 0, 0, 0, 0],\n",
                        " 'labels': [2, 759, 18204, 11, 8274, 15, 10, 3, 0, 0, 0, 0],\n",
                        " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
                    ]
                }
            ],
            "source": [
                "pprint.pprint(encoding)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`encode_plus_untagged()`メソッドは、文章を符号化し、空白や未知語を考慮した上でそれぞれのトークンの位置を返す。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-7\n",
                "\n",
                "wrong_text = \"優勝トロフィーを変換した\"\n",
                "encoding, spans = tokenizer.encode_plus_untagged(\n",
                "    wrong_text, return_tensors='pt'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'input_ids': tensor([    2,   759, 18204,    11,  4618,    15,    10,     3]),\n",
                            " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0]),\n",
                            " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1])}"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[[-1, -1], [0, 2], [2, 7], [7, 8], [8, 10], [10, 11], [11, 12], [-1, -1]]"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "spans"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`convert_output_to_text()`関数は、文章とラベル列、各トークンの文章中の位置から予測した文章を出力する。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-8\n",
                "\n",
                "labels_pred = [2, 759, 18204, 11, 8274, 15, 10, 3]\n",
                "text_pred = tokenizer.convert_output_to_text(\n",
                "    wrong_text, labels_pred, spans\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'優勝トロフィーを返還した'"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "text_pred"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9.2 BERTにおける実装"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "zCFGE4xbGXNe",
                "jupyter": {
                    "outputs_hidden": false
                }
            },
            "outputs": [],
            "source": [
                "# 9-9\n",
                "bert_mlm = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
                "bert_mlm = bert_mlm.cuda()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "J2DR-ls8GXNf"
            },
            "outputs": [],
            "source": [
                "# 9-10\n",
                "text = '優勝トロフィーを変換した。'\n",
                "\n",
                "# 符号化とともに各トークンの文章中の位置を計算しておく。\n",
                "encoding, spans = tokenizer.encode_plus_untagged(\n",
                "    text, return_tensors='pt'\n",
                ")\n",
                "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
                "\n",
                "# BERTに入力し、トークン毎にスコアの最も高いトークンのIDを予測値とする。\n",
                "with torch.no_grad():\n",
                "    output = bert_mlm(**encoding)\n",
                "    scores = output.logits\n",
                "    labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
                "    \n",
                "# ラベル列を文章に変換\n",
                "predict_text = tokenizer.convert_bert_output_to_text(\n",
                "    text, labels_predicted, spans\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Wc0mIVlnGXNf"
            },
            "outputs": [],
            "source": [
                "# 9-11\n",
                "data = [\n",
                "    {\n",
                "        'wrong_text': '優勝トロフィーを変換した。',\n",
                "        'correct_text': '優勝トロフィーを返還した。',\n",
                "    },\n",
                "    {\n",
                "        'wrong_text': '人と森は強制している。',\n",
                "        'correct_text': '人と森は共生している。',\n",
                "    }\n",
                "]\n",
                "\n",
                "# 各データを符号化し、データローダへ入力できるようにする。\n",
                "max_length=32\n",
                "dataset_for_loader = []\n",
                "for sample in data:\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    encoding = tokenizer.encode_plus_tagged(\n",
                "        wrong_text, correct_text, max_length=max_length\n",
                "    )\n",
                "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
                "    dataset_for_loader.append(encoding)\n",
                "\n",
                "# データローダを作成\n",
                "dataloader = DataLoader(dataset_for_loader, batch_size=2)\n",
                "\n",
                "# ミニバッチをBERTへ入力し、損失を計算。\n",
                "for batch in dataloader:\n",
                "    encoding = { k: v.cuda() for k, v in batch.items() }\n",
                "    output = bert_mlm(**encoding)\n",
                "    loss = output.loss # 損失"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9-2\n",
                "\n",
                "os.makedirs(\"/data/chapter9\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "V0KBJLE4GXNg"
            },
            "outputs": [],
            "source": [
                "# 9-12\n",
                "!curl -L \"https://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd.tar.gz&name=JWTD.tar.gz\" -o JWTD.tar.gz\n",
                "!tar zxvf JWTD.tar.gz"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Z-lflu5sGXNg",
                "jupyter": {
                    "outputs_hidden": false
                }
            },
            "outputs": [],
            "source": [
                "# 9-13\n",
                "def create_dataset(data_df):\n",
                "\n",
                "    tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "    def check_token_count(row):\n",
                "        \"\"\"\n",
                "        誤変換の文章と正しい文章でトークンに対応がつくかどうかを判定。\n",
                "        （条件は上の文章を参照）\n",
                "        \"\"\"\n",
                "        wrong_text_tokens = tokenizer.tokenize(row['wrong_text'])\n",
                "        correct_text_tokens = tokenizer.tokenize(row['correct_text'])\n",
                "        if len(wrong_text_tokens) != len(correct_text_tokens):\n",
                "            return False\n",
                "        \n",
                "        diff_count = 0\n",
                "        threthold_count = 2\n",
                "        for wrong_text_token, correct_text_token \\\n",
                "            in zip(wrong_text_tokens, correct_text_tokens):\n",
                "\n",
                "            if wrong_text_token != correct_text_token:\n",
                "                diff_count += 1\n",
                "                if diff_count > threthold_count:\n",
                "                    return False\n",
                "        return True\n",
                "\n",
                "    def normalize(text):\n",
                "        \"\"\"\n",
                "        文字列の正規化\n",
                "        \"\"\"\n",
                "        text = text.strip()\n",
                "        text = unicodedata.normalize('NFKC', text)\n",
                "        return text\n",
                "\n",
                "    # 漢字の誤変換のデータのみを抜き出す。\n",
                "    category_type = 'kanji-conversion'\n",
                "    data_df.query('category == @category_type', inplace=True) \n",
                "    data_df.rename(\n",
                "        columns={'pre_text': 'wrong_text', 'post_text': 'correct_text'}, \n",
                "        inplace=True\n",
                "    )\n",
                "    \n",
                "    # 誤変換と正しい文章をそれぞれ正規化し、\n",
                "    # それらの間でトークン列に対応がつくもののみを抜き出す。\n",
                "    data_df['wrong_text'] = data_df['wrong_text'].map(normalize) \n",
                "    data_df['correct_text'] = data_df['correct_text'].map(normalize)\n",
                "    kanji_conversion_num = len(data_df)\n",
                "    data_df = data_df[data_df.apply(check_token_count, axis=1)]\n",
                "    same_tokens_count_num = len(data_df)\n",
                "    print(\n",
                "        f'- 漢字誤変換の総数：{kanji_conversion_num}',\n",
                "        f'- トークンの対応関係のつく文章の総数: {same_tokens_count_num}',\n",
                "        f'  (全体の{same_tokens_count_num/kanji_conversion_num*100:.0f}%)',\n",
                "        sep = '\\n'\n",
                "    )\n",
                "    return data_df[['wrong_text', 'correct_text']].to_dict(orient='records')\n",
                "\n",
                "# データのロード\n",
                "train_df = pd.read_json(\n",
                "    './jwtd/train.jsonl', orient='records', lines=True\n",
                ")\n",
                "test_df = pd.read_json(\n",
                "    './jwtd/test.jsonl', orient='records', lines=True\n",
                ")\n",
                "\n",
                "# 学習用と検証用データ\n",
                "print('学習と検証用のデータセット：')\n",
                "dataset = create_dataset(train_df)\n",
                "random.shuffle(dataset)\n",
                "n = len(dataset)\n",
                "n_train = int(n*0.8)\n",
                "dataset_train = dataset[:n_train]\n",
                "dataset_val = dataset[n_train:]\n",
                "\n",
                "# テストデータ\n",
                "print('テスト用のデータセット：')\n",
                "dataset_test = create_dataset(test_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "hlr9SXDNGXNh"
            },
            "outputs": [],
            "source": [
                "# 9-14\n",
                "def create_dataset_for_loader(tokenizer, dataset, max_length):\n",
                "    \"\"\"\n",
                "    データセットをデータローダに入力可能な形式にする。\n",
                "    \"\"\"\n",
                "    dataset_for_loader = []\n",
                "    for sample in tqdm(dataset):\n",
                "        wrong_text = sample['wrong_text']\n",
                "        correct_text = sample['correct_text']\n",
                "        encoding = tokenizer.encode_plus_tagged(\n",
                "            wrong_text, correct_text, max_length=max_length\n",
                "        )\n",
                "        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
                "        dataset_for_loader.append(encoding)\n",
                "    return dataset_for_loader\n",
                "\n",
                "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# データセットの作成\n",
                "max_length = 32\n",
                "dataset_train_for_loader = create_dataset_for_loader(\n",
                "    tokenizer, dataset_train, max_length\n",
                ")\n",
                "dataset_val_for_loader = create_dataset_for_loader(\n",
                "    tokenizer, dataset_val, max_length\n",
                ")\n",
                "\n",
                "# データローダの作成\n",
                "dataloader_train = DataLoader(\n",
                "    dataset_train_for_loader, batch_size=32, shuffle=True\n",
                ")\n",
                "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "c-j7R6z0GXNh"
            },
            "outputs": [],
            "source": [
                "# 9-15\n",
                "class BertForMaskedLM_pl(pl.LightningModule):\n",
                "        \n",
                "    def __init__(self, model_name, lr):\n",
                "        super().__init__()\n",
                "        self.save_hyperparameters()\n",
                "        self.bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
                "        \n",
                "    def training_step(self, batch, batch_idx):\n",
                "        output = self.bert_mlm(**batch)\n",
                "        loss = output.loss\n",
                "        self.log('train_loss', loss)\n",
                "        return loss\n",
                "        \n",
                "    def validation_step(self, batch, batch_idx):\n",
                "        output = self.bert_mlm(**batch)\n",
                "        val_loss = output.loss\n",
                "        self.log('val_loss', val_loss)\n",
                "   \n",
                "    def configure_optimizers(self):\n",
                "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
                "\n",
                "checkpoint = pl.callbacks.ModelCheckpoint(\n",
                "    monitor='val_loss',\n",
                "    mode='min',\n",
                "    save_top_k=1,\n",
                "    save_weights_only=True,\n",
                "    dirpath='model/'\n",
                ")\n",
                "\n",
                "trainer = pl.Trainer(\n",
                "    gpus=1,\n",
                "    max_epochs=5,\n",
                "    callbacks=[checkpoint]\n",
                ")\n",
                "\n",
                "# ファインチューニング\n",
                "model = BertForMaskedLM_pl(MODEL_NAME, lr=1e-5)\n",
                "trainer.fit(model, dataloader_train, dataloader_val)\n",
                "best_model_path = checkpoint.best_model_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "w2eNTz6OsvVP"
            },
            "outputs": [],
            "source": [
                "# 9-16\n",
                "def predict(text, tokenizer, bert_mlm):\n",
                "    \"\"\"\n",
                "    文章を入力として受け、BERTが予測した文章を出力\n",
                "    \"\"\"\n",
                "    # 符号化\n",
                "    encoding, spans = tokenizer.encode_plus_untagged(\n",
                "        text, return_tensors='pt'\n",
                "    ) \n",
                "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
                "\n",
                "    # ラベルの予測値の計算\n",
                "    with torch.no_grad():\n",
                "        output = bert_mlm(**encoding)\n",
                "        scores = output.logits\n",
                "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
                "\n",
                "    # ラベル列を文章に変換\n",
                "    predict_text = tokenizer.convert_bert_output_to_text(\n",
                "        text, labels_predicted, spans\n",
                "    )\n",
                "\n",
                "    return predict_text\n",
                "\n",
                "# いくつかの例に対してBERTによる文章校正を行ってみる。\n",
                "text_list = [\n",
                "    'ユーザーの試行に合わせた楽曲を配信する。',\n",
                "    'メールに明日の会議の史料を添付した。',\n",
                "    '乳酸菌で牛乳を発行するとヨーグルトができる。',\n",
                "    '突然、子供が帰省を発した。'\n",
                "]\n",
                "\n",
                "# トークナイザ、ファインチューニング済みのモデルのロード\n",
                "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
                "model = BertForMaskedLM_pl.load_from_checkpoint(best_model_path)\n",
                "bert_mlm = model.bert_mlm.cuda()\n",
                "\n",
                "for text in text_list:\n",
                "    predict_text = predict(text, tokenizer, bert_mlm) # BERTによる予測\n",
                "    print('---')\n",
                "    print(f'入力：{text}')\n",
                "    print(f'出力：{predict_text}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "0zwqdG4SGXNi",
                "jupyter": {
                    "outputs_hidden": false
                }
            },
            "outputs": [],
            "source": [
                "# 9-17\n",
                "# BERTで予測を行い、正解数を数える。\n",
                "correct_num = 0 \n",
                "for sample in tqdm(dataset_test):\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    predict_text = predict(wrong_text, tokenizer, bert_mlm) # BERT予測\n",
                "   \n",
                "    if correct_text == predict_text: # 正解の場合\n",
                "        correct_num += 1\n",
                "\n",
                "print(f'Accuracy: {correct_num/len(dataset_test):.2f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ZUj8v6dPGXNj"
            },
            "outputs": [],
            "source": [
                "# 9-18\n",
                "correct_position_num = 0 # 正しく誤変換の漢字を特定できたデータの数\n",
                "for sample in tqdm(dataset_test):\n",
                "    wrong_text = sample['wrong_text']\n",
                "    correct_text = sample['correct_text']\n",
                "    \n",
                "    # 符号化\n",
                "    encoding = tokenizer(wrong_text)\n",
                "    wrong_input_ids = encoding['input_ids'] # 誤変換の文の符合列\n",
                "    encoding = {k: torch.tensor([v]).cuda() for k,v in encoding.items()}\n",
                "    correct_encoding = tokenizer(correct_text)\n",
                "    correct_input_ids = correct_encoding['input_ids'] # 正しい文の符合列\n",
                "    \n",
                "    # 文章を予測\n",
                "    with torch.no_grad():\n",
                "        output = bert_mlm(**encoding)\n",
                "        scores = output.logits\n",
                "        # 予測された文章のトークンのID\n",
                "        predict_input_ids = scores[0].argmax(-1).cpu().numpy().tolist() \n",
                "\n",
                "    # 特殊トークンを取り除く\n",
                "    wrong_input_ids = wrong_input_ids[1:-1]\n",
                "    correct_input_ids =  correct_input_ids[1:-1]\n",
                "    predict_input_ids =  predict_input_ids[1:-1]\n",
                "    \n",
                "    # 誤変換した漢字を特定できているかを判定\n",
                "    # 符合列を比較する。\n",
                "    detect_flag = True\n",
                "    for wrong_token, correct_token, predict_token \\\n",
                "        in zip(wrong_input_ids, correct_input_ids, predict_input_ids):\n",
                "\n",
                "        if wrong_token == correct_token: # 正しいトークン\n",
                "            # 正しいトークンなのに誤って別のトークンに変換している場合\n",
                "            if wrong_token != predict_token: \n",
                "                detect_flag = False\n",
                "                break\n",
                "        else: # 誤変換のトークン\n",
                "            # 誤変換のトークンなのに、そのままにしている場合\n",
                "            if wrong_token == predict_token: \n",
                "                detect_flag = False\n",
                "                break\n",
                "\n",
                "    if detect_flag: # 誤変換の漢字の位置を正しく特定できた場合\n",
                "        correct_position_num += 1\n",
                "        \n",
                "print(f'Accuracy: {correct_position_num/len(dataset_test):.2f}')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "Chapter9.ipynb",
            "provenance": [
                {
                    "file_id": "https://github.com/stockmarkteam/bert-book/blob/master/Chapter9.ipynb",
                    "timestamp": 1630577154754
                }
            ]
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
