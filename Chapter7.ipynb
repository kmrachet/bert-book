{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWPivw5Ss1Hk"
   },
   "source": [
    "# 7章 マルチクラス分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3yA9qNU_oeAJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eqa5alJ7bax3"
   },
   "source": [
    "本章では、文章のマルチクラス分類を行う。マルチラベル分類は、複数のカテゴリに所属する文章を分類するタスクである。<br>\n",
    "ラベルには、Multi-Hotベクトルと呼ばれるベクトルをあてる。これは、$(0, 1, 1, 0)$といった、One-Hotベクトルの発展的なかたちで、各要素(ラベル)に対しそれぞれフラグがたったものである。$(0, 1, 1, 0)$は、ある文章が4つのカテゴリのうちカテゴリ2と3にあてはまることを意味する。\n",
    "\n",
    "まずは`transformers.BertModel`クラスをベースに`BertForSequenceClassificationMultiLabel`クラスを実装し、その挙動を確認していく。実装に関して、シングルラベル分類とはいくつか異なる点が存在するため注意する。\n",
    "\n",
    "1. 3つ以上のカテゴリを持つシングルラベル分類と違い、損失関数には`BinaryCrossEntropyLoss`を用いる。<br>\n",
    "Multi-Hotベクトルは各カテゴリに対して`0 or 1`で表現されるためである。\n",
    "2. 各カテゴリ(Multi-Hotベクトルの各要素)に対し文章が当てはまる確率を出力するため、>50%の際に1とする実装が必要である。\n",
    "\n",
    "本章では、モデルの最終層の出力をすべてのトークンに対し平均化し、線形変換を適用したものをスコアとする。<br>\n",
    "平均化にあたり、文章長を調整する`[PAD]`トークンを削除する必要がある。`[PAD]`トークンは`encoding`で得られる`attention_mask`で`0`が与えられるため、`attention_mask`が`1`のトークンで平均を取るようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xysfpyHrsHoB"
   },
   "outputs": [],
   "source": [
    "# 7-4\n",
    "\n",
    "class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        # BERTモデルの読み込み\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        # 線形結合の初期化\n",
    "        self.linear = torch.nn.Linear(\n",
    "            self.model.config.hidden_size, num_labels\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor=None, \n",
    "        attention_mask: torch.Tensor=None, \n",
    "        token_type_ids: torch.Tensor=None,\n",
    "        labels: torch.Tensor=None,\n",
    "        ):\n",
    "        # モデルの最終層の出力\n",
    "        model_output = self.model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        # (batch_size, トークン数, 隠れ層数768) のtorch.Tensorを得る\n",
    "        # ここでは(2, 17, 768)\n",
    "        last_hidden_state = model_output.last_hidden_state\n",
    "\n",
    "        # [PAD]トークン以外で平均を取る\n",
    "        # attention_maskはtokenizerより渡された\n",
    "        # (バッチサイズ, トークン数) のOne-Hotベクトルのtorch.Tensor\n",
    "        # ここでは(2, 17)\n",
    "        # torch.Tensor.unsqueeze(axis)メソッドで新規に1の次元を挿入する\n",
    "        # (2, 17) -> (2, 17, 1)\n",
    "        # torch.Tensor.unsqueeze(axis=2)と同義\n",
    "        # last_hidden_stateが3次元ベクトルなので合わせる\n",
    "        # torch.Tensor.sum(axis)で次元に沿って2次元目で足し算する\n",
    "        # (2, 17, 768) -> (2, 768)\n",
    "        # attention_mask.sum(axis=1)でattention_maskが1となる数を取得する\n",
    "        # keepdim=Trueにすることで、sum()で次元が(2, 17) -> (2,)になるのを防ぎ\n",
    "        # 2次元テンソルの形 今回は(2, 1) を維持する\n",
    "        averaged_hidden_state = (\n",
    "            last_hidden_state * attention_mask.unsqueeze(axis=-1)\n",
    "        ).sum(axis=1) / attention_mask.sum(axis=1, keepdim=True)\n",
    "\n",
    "        # 線形結合する\n",
    "        scores = self.linear(averaged_hidden_state)\n",
    "\n",
    "        # lossの計算\n",
    "        output = {\"logits\": scores}\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float()) # float型に変換する必要がある\n",
    "            output[\"loss\"] = loss\n",
    "\n",
    "        # 属性でアクセスできるようにする\n",
    "        # lossにはoutput.lossでアクセスできるようになる\n",
    "        output = type(\"bert_output\", (object,), output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "モデルと文章、ラベルを定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLjvnVuFAUY2",
    "outputId": "2d0bb094-539a-4460-d5de-a907201fc1c3"
   },
   "outputs": [],
   "source": [
    "# 7-5, 7-6\n",
    "\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassificationMultiLabel(\n",
    "    MODEL_NAME, num_labels=2\n",
    ")\n",
    "# model = mode.cuda()\n",
    "\n",
    "# [CLS] + 本文15トークン + [SEP] の最大17トークン\n",
    "# ['今日', 'の', '仕事', 'は', 'うまく', 'いっ', 'た', 'が', '、', '体調', 'が', 'あまり', '良く', 'ない', '。']\n",
    "text_lst = [\n",
    "    '今日の仕事はうまくいったが、体調があまり良くない。', \n",
    "    '昨日は楽しかった。'\n",
    "]\n",
    "\n",
    "# 2次元目は[負の感情, 正の感情]\n",
    "labels_lst = [\n",
    "    [1, 1],\n",
    "    [0, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "トークナイザを定義し予測を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLjvnVuFAUY2",
    "outputId": "2d0bb094-539a-4460-d5de-a907201fc1c3"
   },
   "outputs": [],
   "source": [
    "encoding = tokenizer(text_lst, padding='longest', return_tensors='pt')\n",
    "encoding[\"labels\"] = torch.tensor(labels_lst)\n",
    "# encoding = {key: torch.tensor(value) for key, value in encoding.items()}\n",
    "\n",
    "output = model(**encoding)\n",
    "scores = output.logits\n",
    "labels_pred = (scores > 0).int() # スコアが>0ならTrue、そうでないならFalse\n",
    "accuracy = accuracy_score(labels.numpy(), labels_pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: tensor([[1, 1],\n",
      "        [0, 1]])\n",
      "y_pred: tensor([[0, 1],\n",
      "        [0, 1]], dtype=torch.int32)\n",
      "accuracy: 0.5\n",
      "loss: 0.681\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_true: {labels}\")\n",
    "print(f\"y_pred: {labels_pred}\")\n",
    "print(f\"accuracy: {accuracy}\")\n",
    "print(f\"loss: {output.loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1文目はラベル`[1, 1]`に対し`[0, 1]`と予測してハズレ<br>\n",
    "2文目はラベル`[0, 1]`に対し`[0, 1]`と予測してアタリ<br>\n",
    "総合でAccuracyは0.5となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 chABSA-datasetでマルチクラス分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では、TIS株式会社が公開している、[上場企業の有価証券報告書から作成されたマルチラベルデータセット`chABSA-dataset`](https://github.com/chakki-works/chABSA-dataset)を用いる。\n",
    "\n",
    "このデータセットは、「ネガティブ」「ポジティブ」「ニュートラル」という3クラスを用いる。文章に対してそれぞれのクラスに該当する表現があると、カテゴリーと何を対象としているかをラベル付けしている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/chapter7\", exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  705k  100  705k    0     0  1524k      0 --:--:-- --:--:-- --:--:-- 1541k\n",
      "Archive:  data/chapter7/chABSA-dataset.zip\n",
      "   creating: chABSA-dataset/\n",
      "  inflating: chABSA-dataset/.DS_Store  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/chABSA-dataset/\n",
      "  inflating: __MACOSX/chABSA-dataset/._.DS_Store  \n",
      " extracting: chABSA-dataset/.gitkeep  \n",
      "  inflating: chABSA-dataset/e00008_ann.json  \n",
      "  inflating: chABSA-dataset/e00017_ann.json  \n",
      "  inflating: chABSA-dataset/e00024_ann.json  \n",
      "  inflating: chABSA-dataset/e00026_ann.json  \n",
      "  inflating: chABSA-dataset/e00030_ann.json  \n",
      "  inflating: chABSA-dataset/e00033_ann.json  \n",
      "  inflating: chABSA-dataset/e00034_ann.json  \n",
      "  inflating: chABSA-dataset/e00035_ann.json  \n",
      "  inflating: chABSA-dataset/e00037_ann.json  \n",
      "  inflating: chABSA-dataset/e00051_ann.json  \n",
      "  inflating: chABSA-dataset/e00053_ann.json  \n",
      "  inflating: chABSA-dataset/e00058_ann.json  \n",
      "  inflating: chABSA-dataset/e00069_ann.json  \n",
      "  inflating: chABSA-dataset/e00091_ann.json  \n",
      "  inflating: chABSA-dataset/e00107_ann.json  \n",
      "  inflating: chABSA-dataset/e00114_ann.json  \n",
      "  inflating: chABSA-dataset/e00146_ann.json  \n",
      "  inflating: chABSA-dataset/e00168_ann.json  \n",
      "  inflating: chABSA-dataset/e00184_ann.json  \n",
      "  inflating: chABSA-dataset/e00194_ann.json  \n",
      "  inflating: chABSA-dataset/e00273_ann.json  \n",
      "  inflating: chABSA-dataset/e00308_ann.json  \n",
      "  inflating: chABSA-dataset/e00343_ann.json  \n",
      "  inflating: chABSA-dataset/e00354_ann.json  \n",
      "  inflating: chABSA-dataset/e00380_ann.json  \n",
      "  inflating: chABSA-dataset/e00406_ann.json  \n",
      "  inflating: chABSA-dataset/e00435_ann.json  \n",
      "  inflating: chABSA-dataset/e00457_ann.json  \n",
      "  inflating: chABSA-dataset/e00465_ann.json  \n",
      "  inflating: chABSA-dataset/e00501_ann.json  \n",
      "  inflating: chABSA-dataset/e00534_ann.json  \n",
      "  inflating: chABSA-dataset/e00541_ann.json  \n",
      "  inflating: chABSA-dataset/e00547_ann.json  \n",
      "  inflating: chABSA-dataset/e00563_ann.json  \n",
      "  inflating: chABSA-dataset/e00603_ann.json  \n",
      "  inflating: chABSA-dataset/e00686_ann.json  \n",
      "  inflating: chABSA-dataset/e00693_ann.json  \n",
      "  inflating: chABSA-dataset/e00694_ann.json  \n",
      "  inflating: chABSA-dataset/e00721_ann.json  \n",
      "  inflating: chABSA-dataset/e00772_ann.json  \n",
      "  inflating: chABSA-dataset/e00787_ann.json  \n",
      "  inflating: chABSA-dataset/e00810_ann.json  \n",
      "  inflating: chABSA-dataset/e00832_ann.json  \n",
      "  inflating: chABSA-dataset/e00838_ann.json  \n",
      "  inflating: chABSA-dataset/e00840_ann.json  \n",
      "  inflating: chABSA-dataset/e00842_ann.json  \n",
      "  inflating: chABSA-dataset/e00858_ann.json  \n",
      "  inflating: chABSA-dataset/e00877_ann.json  \n",
      "  inflating: chABSA-dataset/e00886_ann.json  \n",
      "  inflating: chABSA-dataset/e00909_ann.json  \n",
      "  inflating: chABSA-dataset/e00911_ann.json  \n",
      "  inflating: chABSA-dataset/e00939_ann.json  \n",
      "  inflating: chABSA-dataset/e00962_ann.json  \n",
      "  inflating: chABSA-dataset/e00976_ann.json  \n",
      "  inflating: chABSA-dataset/e01018_ann.json  \n",
      "  inflating: chABSA-dataset/e01043_ann.json  \n",
      "  inflating: chABSA-dataset/e01054_ann.json  \n",
      "  inflating: chABSA-dataset/e01097_ann.json  \n",
      "  inflating: chABSA-dataset/e01118_ann.json  \n",
      "  inflating: chABSA-dataset/e01151_ann.json  \n",
      "  inflating: chABSA-dataset/e01156_ann.json  \n",
      "  inflating: chABSA-dataset/e01173_ann.json  \n",
      "  inflating: chABSA-dataset/e01183_ann.json  \n",
      "  inflating: chABSA-dataset/e01197_ann.json  \n",
      "  inflating: chABSA-dataset/e01216_ann.json  \n",
      "  inflating: chABSA-dataset/e01230_ann.json  \n",
      "  inflating: chABSA-dataset/e01244_ann.json  \n",
      "  inflating: chABSA-dataset/e01249_ann.json  \n",
      "  inflating: chABSA-dataset/e01260_ann.json  \n",
      "  inflating: chABSA-dataset/e01334_ann.json  \n",
      "  inflating: chABSA-dataset/e01364_ann.json  \n",
      "  inflating: chABSA-dataset/e01398_ann.json  \n",
      "  inflating: chABSA-dataset/e01402_ann.json  \n",
      "  inflating: chABSA-dataset/e01425_ann.json  \n",
      "  inflating: chABSA-dataset/e01436_ann.json  \n",
      "  inflating: chABSA-dataset/e01462_ann.json  \n",
      "  inflating: chABSA-dataset/e01469_ann.json  \n",
      "  inflating: chABSA-dataset/e01506_ann.json  \n",
      "  inflating: chABSA-dataset/e01528_ann.json  \n",
      "  inflating: chABSA-dataset/e01529_ann.json  \n",
      "  inflating: chABSA-dataset/e01533_ann.json  \n",
      "  inflating: chABSA-dataset/e01546_ann.json  \n",
      "  inflating: chABSA-dataset/e01585_ann.json  \n",
      "  inflating: chABSA-dataset/e01620_ann.json  \n",
      "  inflating: chABSA-dataset/e01624_ann.json  \n",
      "  inflating: chABSA-dataset/e01629_ann.json  \n",
      "  inflating: chABSA-dataset/e01635_ann.json  \n",
      "  inflating: chABSA-dataset/e01703_ann.json  \n",
      "  inflating: chABSA-dataset/e01719_ann.json  \n",
      "  inflating: chABSA-dataset/e01731_ann.json  \n",
      "  inflating: chABSA-dataset/e01740_ann.json  \n",
      "  inflating: chABSA-dataset/e01743_ann.json  \n",
      "  inflating: chABSA-dataset/e01764_ann.json  \n",
      "  inflating: chABSA-dataset/e01794_ann.json  \n",
      "  inflating: chABSA-dataset/e01798_ann.json  \n",
      "  inflating: chABSA-dataset/e01813_ann.json  \n",
      "  inflating: chABSA-dataset/e01849_ann.json  \n",
      "  inflating: chABSA-dataset/e01862_ann.json  \n",
      "  inflating: chABSA-dataset/e01865_ann.json  \n",
      "  inflating: chABSA-dataset/e01903_ann.json  \n",
      "  inflating: chABSA-dataset/e01904_ann.json  \n",
      "  inflating: chABSA-dataset/e01933_ann.json  \n",
      "  inflating: chABSA-dataset/e01946_ann.json  \n",
      "  inflating: chABSA-dataset/e01968_ann.json  \n",
      "  inflating: chABSA-dataset/e01972_ann.json  \n",
      "  inflating: chABSA-dataset/e01974_ann.json  \n",
      "  inflating: chABSA-dataset/e01987_ann.json  \n",
      "  inflating: chABSA-dataset/e01992_ann.json  \n",
      "  inflating: chABSA-dataset/e02009_ann.json  \n",
      "  inflating: chABSA-dataset/e02049_ann.json  \n",
      "  inflating: chABSA-dataset/e02105_ann.json  \n",
      "  inflating: chABSA-dataset/e02150_ann.json  \n",
      "  inflating: chABSA-dataset/e02152_ann.json  \n",
      "  inflating: chABSA-dataset/e02214_ann.json  \n",
      "  inflating: chABSA-dataset/e02241_ann.json  \n",
      "  inflating: chABSA-dataset/e02246_ann.json  \n",
      "  inflating: chABSA-dataset/e02289_ann.json  \n",
      "  inflating: chABSA-dataset/e02353_ann.json  \n",
      "  inflating: chABSA-dataset/e02367_ann.json  \n",
      "  inflating: chABSA-dataset/e02380_ann.json  \n",
      "  inflating: chABSA-dataset/e02382_ann.json  \n",
      "  inflating: chABSA-dataset/e02390_ann.json  \n",
      "  inflating: chABSA-dataset/e02414_ann.json  \n",
      "  inflating: chABSA-dataset/e02423_ann.json  \n",
      "  inflating: chABSA-dataset/e02505_ann.json  \n",
      "  inflating: chABSA-dataset/e02525_ann.json  \n",
      "  inflating: chABSA-dataset/e02530_ann.json  \n",
      "  inflating: chABSA-dataset/e02544_ann.json  \n",
      "  inflating: chABSA-dataset/e02547_ann.json  \n",
      "  inflating: chABSA-dataset/e02563_ann.json  \n",
      "  inflating: chABSA-dataset/e02567_ann.json  \n",
      "  inflating: chABSA-dataset/e02608_ann.json  \n",
      "  inflating: chABSA-dataset/e02627_ann.json  \n",
      "  inflating: chABSA-dataset/e02632_ann.json  \n",
      "  inflating: chABSA-dataset/e02643_ann.json  \n",
      "  inflating: chABSA-dataset/e02673_ann.json  \n",
      "  inflating: chABSA-dataset/e02682_ann.json  \n",
      "  inflating: chABSA-dataset/e02732_ann.json  \n",
      "  inflating: chABSA-dataset/e02825_ann.json  \n",
      "  inflating: chABSA-dataset/e02837_ann.json  \n",
      "  inflating: chABSA-dataset/e02889_ann.json  \n",
      "  inflating: chABSA-dataset/e02905_ann.json  \n",
      "  inflating: chABSA-dataset/e02946_ann.json  \n",
      "  inflating: chABSA-dataset/e03128_ann.json  \n",
      "  inflating: chABSA-dataset/e03226_ann.json  \n",
      "  inflating: chABSA-dataset/e03236_ann.json  \n",
      "  inflating: chABSA-dataset/e03267_ann.json  \n",
      "  inflating: chABSA-dataset/e03275_ann.json  \n",
      "  inflating: chABSA-dataset/e03281_ann.json  \n",
      "  inflating: chABSA-dataset/e03367_ann.json  \n",
      "  inflating: chABSA-dataset/e03398_ann.json  \n",
      "  inflating: chABSA-dataset/e03401_ann.json  \n",
      "  inflating: chABSA-dataset/e03472_ann.json  \n",
      "  inflating: chABSA-dataset/e03505_ann.json  \n",
      "  inflating: chABSA-dataset/e03556_ann.json  \n",
      "  inflating: chABSA-dataset/e03566_ann.json  \n",
      "  inflating: chABSA-dataset/e03580_ann.json  \n",
      "  inflating: chABSA-dataset/e03582_ann.json  \n",
      "  inflating: chABSA-dataset/e03584_ann.json  \n",
      "  inflating: chABSA-dataset/e03601_ann.json  \n",
      "  inflating: chABSA-dataset/e03641_ann.json  \n",
      "  inflating: chABSA-dataset/e03693_ann.json  \n",
      "  inflating: chABSA-dataset/e03723_ann.json  \n",
      "  inflating: chABSA-dataset/e03784_ann.json  \n",
      "  inflating: chABSA-dataset/e03929_ann.json  \n",
      "  inflating: chABSA-dataset/e03991_ann.json  \n",
      "  inflating: chABSA-dataset/e04078_ann.json  \n",
      "  inflating: chABSA-dataset/e04091_ann.json  \n",
      "  inflating: chABSA-dataset/e04123_ann.json  \n",
      "  inflating: chABSA-dataset/e04136_ann.json  \n",
      "  inflating: chABSA-dataset/e04147_ann.json  \n",
      "  inflating: chABSA-dataset/e04191_ann.json  \n",
      "  inflating: chABSA-dataset/e04242_ann.json  \n",
      "  inflating: chABSA-dataset/e04273_ann.json  \n",
      "  inflating: chABSA-dataset/e04291_ann.json  \n",
      "  inflating: chABSA-dataset/e04298_ann.json  \n",
      "  inflating: chABSA-dataset/e04304_ann.json  \n",
      "  inflating: chABSA-dataset/e04319_ann.json  \n",
      "  inflating: chABSA-dataset/e04329_ann.json  \n",
      "  inflating: chABSA-dataset/e04331_ann.json  \n",
      "  inflating: chABSA-dataset/e04360_ann.json  \n",
      "  inflating: chABSA-dataset/e04503_ann.json  \n",
      "  inflating: chABSA-dataset/e04509_ann.json  \n",
      "  inflating: chABSA-dataset/e04727_ann.json  \n",
      "  inflating: chABSA-dataset/e04768_ann.json  \n",
      "  inflating: chABSA-dataset/e04844_ann.json  \n",
      "  inflating: chABSA-dataset/e04856_ann.json  \n",
      "  inflating: chABSA-dataset/e04858_ann.json  \n",
      "  inflating: chABSA-dataset/e04867_ann.json  \n",
      "  inflating: chABSA-dataset/e04877_ann.json  \n",
      "  inflating: chABSA-dataset/e04948_ann.json  \n",
      "  inflating: chABSA-dataset/e04976_ann.json  \n",
      "  inflating: chABSA-dataset/e04995_ann.json  \n",
      "  inflating: chABSA-dataset/e05011_ann.json  \n",
      "  inflating: chABSA-dataset/e05110_ann.json  \n",
      "  inflating: chABSA-dataset/e05145_ann.json  \n",
      "  inflating: chABSA-dataset/e05155_ann.json  \n",
      "  inflating: chABSA-dataset/e05167_ann.json  \n",
      "  inflating: chABSA-dataset/e05302_ann.json  \n",
      "  inflating: chABSA-dataset/e05319_ann.json  \n",
      "  inflating: chABSA-dataset/e05322_ann.json  \n",
      "  inflating: chABSA-dataset/e05346_ann.json  \n",
      "  inflating: chABSA-dataset/e05452_ann.json  \n",
      "  inflating: chABSA-dataset/e05469_ann.json  \n",
      "  inflating: chABSA-dataset/e05480_ann.json  \n",
      "  inflating: chABSA-dataset/e05593_ann.json  \n",
      "  inflating: chABSA-dataset/e05629_ann.json  \n",
      "  inflating: chABSA-dataset/e05714_ann.json  \n",
      "  inflating: chABSA-dataset/e05737_ann.json  \n",
      "  inflating: chABSA-dataset/e07801_ann.json  \n",
      "  inflating: chABSA-dataset/e21200_ann.json  \n",
      "  inflating: chABSA-dataset/e21261_ann.json  \n",
      "  inflating: chABSA-dataset/e23818_ann.json  \n",
      "  inflating: chABSA-dataset/e25665_ann.json  \n",
      "  inflating: chABSA-dataset/e26332_ann.json  \n",
      "  inflating: chABSA-dataset/e26443_ann.json  \n",
      "  inflating: chABSA-dataset/e26454_ann.json  \n",
      "  inflating: chABSA-dataset/e26713_ann.json  \n",
      "  inflating: chABSA-dataset/e26914_ann.json  \n",
      "  inflating: chABSA-dataset/e27050_ann.json  \n",
      "  inflating: chABSA-dataset/e27633_ann.json  \n",
      "  inflating: chABSA-dataset/e27759_ann.json  \n",
      "  inflating: chABSA-dataset/e30066_ann.json  \n",
      "  inflating: chABSA-dataset/e30085_ann.json  \n",
      "  inflating: chABSA-dataset/e30479_ann.json  \n",
      "  inflating: chABSA-dataset/e30746_ann.json  \n",
      "  inflating: chABSA-dataset/e32161_ann.json  \n",
      "  inflating: chABSA-dataset/e32189_ann.json  \n",
      "  inflating: chABSA-dataset/e32458_ann.json  \n",
      "  inflating: chABSA-dataset/e33009_ann.json  \n"
     ]
    }
   ],
   "source": [
    "!curl --output \"data/chapter7/chABSA-dataset.zip\" \"https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip\"\n",
    "!unzip -q -d \"data/chapter7\" \"data/chapter7/chABSA-dataset.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSDkkZf0AT8v"
   },
   "source": [
    "それぞれのファイルは`chABSA-dataset/e*****_ann.json`で、全体で230ある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgXcOtz6fLge"
   },
   "outputs": [],
   "source": [
    "# 7-9\n",
    "data = json.load(open('chABSA-dataset/e00030_ann.json'))\n",
    "print( data['sentences'][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l33ix4WDIhtG"
   },
   "outputs": [],
   "source": [
    "# 7-10\n",
    "category_id = {'negative':0, 'neutral':1 , 'positive':2}\n",
    "\n",
    "dataset = []\n",
    "for file in glob.glob('chABSA-dataset/*.json'):\n",
    "    data = json.load(open(file))\n",
    "    # 各データから文章（text）を抜き出し、ラベル（'labels'）を作成\n",
    "    for sentence in data['sentences']:\n",
    "        text = sentence['sentence'] \n",
    "        labels = [0,0,0]\n",
    "        for opinion in sentence['opinions']:\n",
    "            labels[category_id[opinion['polarity']]] = 1\n",
    "        sample = {'text': text, 'labels': labels}\n",
    "        dataset.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4Na8gOPHhya"
   },
   "outputs": [],
   "source": [
    "# 7-11\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igPtmux1IhtI"
   },
   "outputs": [],
   "source": [
    "# 7-12\n",
    "# トークナイザのロード\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 各データの形式を整える\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "for sample in dataset:\n",
    "    text = sample['text']\n",
    "    labels = sample['labels']\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "    encoding['labels'] = labels\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# データセットの分割\n",
    "random.shuffle(dataset_for_loader) \n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "dataset_train = dataset_for_loader[:n_train] # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n",
    "\n",
    "#　データセットからデータローダを作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=32, shuffle=True\n",
    ") \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9y3dO-kBIhtI"
   },
   "outputs": [],
   "source": [
    "# 7-13\n",
    "class BertForSequenceClassificationMultiLabel_pl(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        self.bert_scml = BertForSequenceClassificationMultiLabel(\n",
    "            model_name, num_labels=num_labels\n",
    "        ) \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_scml(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels')\n",
    "        output = self.bert_scml(**batch)\n",
    "        scores = output.logits\n",
    "        labels_predicted = ( scores > 0 ).int()\n",
    "        num_correct = ( labels_predicted == labels ).all(-1).sum().item()\n",
    "        accuracy = num_correct/scores.size(0)\n",
    "        self.log('accuracy', accuracy)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=5,\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassificationMultiLabel_pl(\n",
    "    MODEL_NAME, \n",
    "    num_labels=3, \n",
    "    lr=1e-5\n",
    ")\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My3WI8Qd7yVJ"
   },
   "outputs": [],
   "source": [
    "# 7-14\n",
    "# 入力する文章\n",
    "text_list = [\n",
    "    \"今期は売り上げが順調に推移したが、株価は低迷の一途を辿っている。\",\n",
    "    \"昨年から黒字が減少した。\",\n",
    "    \"今日の飲み会は楽しかった。\"\n",
    "]\n",
    "\n",
    "# モデルのロード\n",
    "best_model_path = checkpoint.best_model_path\n",
    "model = BertForSequenceClassificationMultiLabel_pl.load_from_checkpoint(best_model_path)\n",
    "bert_scml = model.bert_scml.cuda()\n",
    "\n",
    "# データの符号化\n",
    "encoding = tokenizer(\n",
    "    text_list, \n",
    "    padding = 'longest',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "# BERTへデータを入力し分類スコアを得る。\n",
    "with torch.no_grad():\n",
    "    output = bert_scml(**encoding)\n",
    "scores = output.logits\n",
    "labels_predicted = ( scores > 0 ).int().cpu().numpy().tolist()\n",
    "\n",
    "# 結果を表示\n",
    "for text, label in zip(text_list, labels_predicted):\n",
    "    print('--')\n",
    "    print(f'入力：{text}')\n",
    "    print(f'出力：{label}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
