{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKcIdYD2sySs"
   },
   "source": [
    "# 6章\n",
    "- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9l6zMxE0_HBl"
   },
   "source": [
    "実際のデータを用いてBERTのファインチューニングを行い、その性能を評価するということをやっていく。本章では、livedoorニュースコーパスデータセットを用いて、ニュース記事を9つのカテゴリーに分類する、文章分類のタスクにチャレンジする。\n",
    "\n",
    "また、本章ではPytorchのラッパーであるPytorch Lightningを用いて実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 文章分類を試してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-NARPjxVNGF"
   },
   "source": [
    "文章分類には、`transformsers.BertForSequenceClassification`を用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2uwJLtKBUdn7"
   },
   "outputs": [],
   "source": [
    "# 6-3\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBNp1ABCFPQD"
   },
   "source": [
    "まずはテスト的に2値分類を試みる。<br>\n",
    "分類モデルには、予め予測するクラス数を`num_labels`引数に指定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cS06dXRgVqD3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# GPUに載せる\n",
    "# model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgHMUmYrFwsq"
   },
   "source": [
    "分類する文章とそのラベル(正解データ)を用意する。<br>\n",
    "`label_list`変数にはカテゴリ値を代入する。二値分類の場合`True`か`False`かの2値となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XpuQ60wnF_HJ"
   },
   "outputs": [],
   "source": [
    "# 6-5 6-6\n",
    "\n",
    "text_lst = [\n",
    "    \"この映画は面白かった。\",\n",
    "    \"この映画の最後にはがっかりさせられた。\",\n",
    "    \"この映画を見て幸せな気持ちになった。\"\n",
    "]\n",
    "\n",
    "label_lst = [1, 0, 1] # 0: False, 1: True\n",
    "labels = torch.tensor(label_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0hJ6Kn_YHKf"
   },
   "source": [
    "符号化する際に、`labels`変数を辞書型を一緒に代入することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ML-GzvjKWzcK"
   },
   "outputs": [],
   "source": [
    "# 符号化する\n",
    "encoding = tokenizer(text_lst, padding='longest', return_tensors='pt')\n",
    "encoding['labels'] = labels\n",
    "\n",
    "# GPUに載せる\n",
    "# encoding = {key: value.cuda() for key, value in encoding.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFhF4bEMYWlC"
   },
   "source": [
    "`loss`を一緒に出したいので`torch.no_grad()`でブロック化する必要はなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "68p21x0-YchE"
   },
   "outputs": [],
   "source": [
    "# 推論\n",
    "output = model.forward(**encoding)\n",
    "\n",
    "# 結果計算\n",
    "scores = output.logits\n",
    "labels_pred = scores.argmax(axis=1).cpu()\n",
    "num_correct = (labels_pred == labels).sum().item()\n",
    "accuracy = accuracy_score(labels.numpy(), labels_pred.numpy())\n",
    "\n",
    "# 損失\n",
    "loss = output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1686016489211,
     "user": {
      "displayName": "Michito KIMURA",
      "userId": "05325367397627444250"
     },
     "user_tz": -540
    },
    "id": "qtk-XP27Y-WQ",
    "outputId": "7f1e3685-c826-470d-d791-b703160485b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: tensor([1, 0, 1])\n",
      "labels pred: tensor([1, 0, 1])\n",
      "number of correct: 3\n",
      "accuracy: 1.0\n",
      "loss: 0.6269634366035461\n"
     ]
    }
   ],
   "source": [
    "print(f\"labels: {labels}\")\n",
    "print(f\"labels pred: {labels_pred}\")\n",
    "print(f\"number of correct: {num_correct}\")\n",
    "print(f\"accuracy: {accuracy}\")\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1TgAkdqTogm"
   },
   "source": [
    "## 6.4 ファインチューニングと文章分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX65srArR2-G"
   },
   "source": [
    "[株式会社ロンウイットにより公開されているデータセット](https://www.rondhuit.com/download.html#ldcc)を用いる。全部で9つのカテゴリーに分類する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEuHKGJsR2zc"
   },
   "source": [
    "|category|english|n|label|\n",
    "|:---|:---|:---:|:---:|\n",
    "|独女通信|dokujo-tsushin|870|0|\n",
    "|ITライフハック|it-life-hack|870|1|\n",
    "|家電チャンネル|kaden-channel|864|2|\n",
    "|livedoor HOMME|livedoor-homme|511|3|\n",
    "|MOVIE ENTER|movie-enter|870|4|\n",
    "|Peachy|peachy|842|5|\n",
    "|エスマックス|smax|870|6|\n",
    "|Sports Watch|sports-watch|900|7|\n",
    "|トピックニュース|topic-news|770|8|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "V_BGiKTflI39"
   },
   "outputs": [],
   "source": [
    "# 6-3\n",
    "\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm # プログレスバーの表示\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリの作成\n",
    "\n",
    "os.makedirs(\"data/chapter6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl --output data/chapter6/ldcc-20140209.tar.gz https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "!cd data/chapter6 && tar -zxvf ldcc-20140209.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49pchD2z6JhM"
   },
   "outputs": [],
   "source": [
    "# 6-9\n",
    "# データローダーの作成\n",
    "dataset_for_loader = [\n",
    "    {'data':torch.tensor([0,1]), 'labels':torch.tensor(0)},\n",
    "    {'data':torch.tensor([2,3]), 'labels':torch.tensor(1)},\n",
    "    {'data':torch.tensor([4,5]), 'labels':torch.tensor(2)},\n",
    "    {'data':torch.tensor([6,7]), 'labels':torch.tensor(3)},\n",
    "]\n",
    "loader = DataLoader(dataset_for_loader, batch_size=2)\n",
    "\n",
    "# データセットからミニバッチを取り出す\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f'# batch {idx}')\n",
    "    print(batch)\n",
    "    ## ファインチューニングではここでミニバッチ毎の処理を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_1f6IbMVbaH"
   },
   "outputs": [],
   "source": [
    "# 6-10\n",
    "loader = DataLoader(dataset_for_loader, batch_size=2, shuffle=True)\n",
    "\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f'# batch {idx}')\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9YGEfZUAxea"
   },
   "outputs": [],
   "source": [
    "# 6-11\n",
    "# カテゴリーのリスト\n",
    "category_list = [\n",
    "    'dokujo-tsushin',\n",
    "    'it-life-hack',\n",
    "    'kaden-channel',\n",
    "    'livedoor-homme',\n",
    "    'movie-enter',\n",
    "    'peachy',\n",
    "    'smax',\n",
    "    'sports-watch',\n",
    "    'topic-news'\n",
    "]\n",
    "\n",
    "# トークナイザのロード\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 各データの形式を整える\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "for label, category in enumerate(tqdm(category_list)):\n",
    "    for file in glob.glob(f'./text/{category}/{category}*'):\n",
    "        lines = open(file).read().splitlines()\n",
    "        text = '\\n'.join(lines[3:]) # ファイルの4行目からを抜き出す。\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length, \n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        encoding['labels'] = label # ラベルを追加\n",
    "        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "        dataset_for_loader.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drP8IYLVBFh_"
   },
   "outputs": [],
   "source": [
    "# 6-12\n",
    "print(dataset_for_loader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHY9Os6NJlip"
   },
   "outputs": [],
   "source": [
    "# 6-13\n",
    "# データセットの分割\n",
    "random.shuffle(dataset_for_loader) # ランダムにシャッフル\n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "dataset_train = dataset_for_loader[:n_train] # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n",
    "\n",
    "# データセットからデータローダを作成\n",
    "# 学習データはshuffle=Trueにする。\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=32, shuffle=True\n",
    ") \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffaUyGcoVj8l"
   },
   "outputs": [],
   "source": [
    "# 6-14\n",
    "class BertForSequenceClassification_pl(pl.LightningModule):\n",
    "        \n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        # model_name: Transformersのモデルの名前\n",
    "        # num_labels: ラベルの数\n",
    "        # lr: 学習率\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # 引数のnum_labelsとlrを保存。\n",
    "        # 例えば、self.hparams.lrでlrにアクセスできる。\n",
    "        # チェックポイント作成時にも自動で保存される。\n",
    "        self.save_hyperparameters() \n",
    "\n",
    "        # BERTのロード\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        \n",
    "    # 学習データのミニバッチ(`batch`)が与えられた時に損失を出力する関数を書く。\n",
    "    # batch_idxはミニバッチの番号であるが今回は使わない。\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss) # 損失を'train_loss'の名前でログをとる。\n",
    "        return loss\n",
    "        \n",
    "    # 検証データのミニバッチが与えられた時に、\n",
    "    # 検証データを評価する指標を計算する関数を書く。\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss) # 損失を'val_loss'の名前でログをとる。\n",
    "\n",
    "    # テストデータのミニバッチが与えられた時に、\n",
    "    # テストデータを評価する指標を計算する関数を書く。\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels') # バッチからラベルを取得\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = ( labels_predicted == labels ).sum().item()\n",
    "        accuracy = num_correct/labels.size(0) #精度\n",
    "        self.log('accuracy', accuracy) # 精度を'accuracy'の名前でログをとる。\n",
    "\n",
    "    # 学習に用いるオプティマイザを返す関数を書く。\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyR6de1TqfW9"
   },
   "outputs": [],
   "source": [
    "# 6-15\n",
    "# 学習時にモデルの重みを保存する条件を指定\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/',\n",
    ")\n",
    "\n",
    "# 学習の方法を指定\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=10,\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgk48zEqIJKh"
   },
   "outputs": [],
   "source": [
    "# 6-16\n",
    "# PyTorch Lightningモデルのロード\n",
    "model = BertForSequenceClassification_pl(\n",
    "    MODEL_NAME, num_labels=9, lr=1e-5\n",
    ")\n",
    "\n",
    "# ファインチューニングを行う。\n",
    "trainer.fit(model, dataloader_train, dataloader_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h68P7MG-JSh9"
   },
   "outputs": [],
   "source": [
    "# 6-17\n",
    "best_model_path = checkpoint.best_model_path # ベストモデルのファイル\n",
    "print('ベストモデルのファイル: ', checkpoint.best_model_path)\n",
    "print('ベストモデルの検証データに対する損失: ', checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-r9stqZqBdW"
   },
   "outputs": [],
   "source": [
    "# 6-18\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bx0L0Ehr1tM"
   },
   "outputs": [],
   "source": [
    "# 6-19\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbJAUdrStSgI"
   },
   "outputs": [],
   "source": [
    "# 6-20\n",
    "# PyTorch Lightningモデルのロード\n",
    "model = BertForSequenceClassification_pl.load_from_checkpoint(\n",
    "    best_model_path\n",
    ") \n",
    "\n",
    "# Transformers対応のモデルを./model_transformesに保存\n",
    "model.bert_sc.save_pretrained('./model_transformers') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcho1B0BtfV0"
   },
   "outputs": [],
   "source": [
    "# 6-21\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "    './model_transformers'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/stockmarkteam/bert-book/blob/master/Chapter6.ipynb",
     "timestamp": 1630571793610
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
